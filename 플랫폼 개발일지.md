# 플랫폼 개발일지


#### < 1주차 >

- 쿠버네티스 컨테이너디 환경 세팅 ( 완료 )
- ELK 설치 ( 완료 )
- 파이프라인 시나리오 작성 ( 진행중 ) 
- 카프카 올리기 ( 완료 )
- 하둡 이미지 빌드하기 ( 완료 )
- 하둡 이미지 위에 스파크 올려서 이미지 빌드하기 ( 완료 )
- 하둡 + 스파크 이미지 올리기 ( 완료 )
- 하둡 + 스파크 이미지 위에 제플린 올려서 이미지 빌드하기 ( 완료 )
- 하둡 + 스파크 + 제플린 이미지 사용해서 디플로이먼트 올리기 ( 완료 )
- 쿠버네티스 디플로이먼트 포트포워딩 고려 ( 완료 )
- 프로그램 개발환경 테스트 ( 진행중 )
- 프로그램들 모두 연결하기 ( 추후과제 )
- 쿠버네티스 컨테이너 기능들 추가해보기 ( 보류 )



**2022 / 05 / 03**

- Containerd 설치
- Kubernetes 설치

	master node 1대  
	worker node 5대

	master 172.30.1.222  
	worker1 172.30.1.2  
	worker2 172.30.1.62  
	worker3 172.30.1.129  
	worker4 172.30.1.144  
	worker5 172.30.1.194  

▶ 오늘의 오류 발생 및 해결 과정
	
	1. calico node 0/1 문제 발생
		: 방화벽 문제인줄 알고 방화벽 해제 했지만 계속 같은 문제 발생
		  → 가상머신 Network를 Bridge로 설정하지 않고 NAT로 한 것이 원인

**2022 / 05 / 04**

- Elasticsearch 설치
- Logstash 설치
- Kibana 설치
- Spark 설치


▶ 오늘의 오류 발생 및 해결 과정

	1. apt install docker.io 설치 오류
		→ apt install containerd 실행하고 다시 apt install docker.io 진행하기

**2022 / 05 / 05**

- Ubuntu 이미지 위에 Hadoop 설치

▶ 오늘의 오류 발생 및 해결 과정

	1. ssh 연결 오류
		→ vi /etc/hosts.allow 파일에 ssh:ALL:allow\sshd:ALL:allow 추가 후
                             service ssh restart 명령어 실행

	2. 도커 이미지 push 시 requeste denied 오류 발생
		→ docker login이 안되어 있거나 이미지 username과 docker hub id가 일치하지 않을 때 발생

**2022 / 05 / 06**

- Ubuntu 이미지 위에 kafka 설치

▶ 오늘의 오류 발생 및 해결 과정

	1. /opt/kafka/bin/zookeeper-server-start.sh config/zookeeper.properties 실행이 안됨
		→ producer와 consumer가 아직 준비 되지 않아서 설치로 끝냄

**2022 / 05 / 07**

- Ubuntu + Hadoop 이미지 위에 Spark 설치

▶ 오늘의 오류 발생 및 해결 과정

	1. bash : start command not found
		→ .bashrc 에 환경변수 설정을 안넣어줘서 발생하는 오류

**2022 / 05 / 08**

- Ubuntu + Hadoop + Spark 이미지 위에 Zepplin 설치

▶ 오늘의 오류 발생 및 해결 과정

	1. wget으로 zeppelin 파일을 다운받을 때 root 디렉토리에 저장하지 않아서 경로 설정에 충돌 발생 

	2. 컨테이너 자체를 접속할 때 포트포워딩을 하고 접속을 하면 되는데 디플로이먼트로 생성 후 포트포워딩을 어떻게 해야할지 생각해보기  





#### < 2주차 >

- 하둡&스파크 / 제플린 컨테이너 분리 ( 완료 )
- hadoop, spark, zeppelin on k8s 구현 ( 완료 )
- 카프카 & 로그스태시 연결 ( 완료 )
- 뉴스 크롤링 데이터를 통한 프로그램 간 데이터 이동 테스트 ( 연기 )
- helm 사용하기 ( 취소 )


**2022 / 05 / 09**

- hadoop, spark clustering 구현 진행중
- Kafka & Logstash 연결
- helm을 프로젝트 환경에 맞게 사용하기 (불필요한 chart 삭제 등, 차지하는 데이터가 많으면 사용 보류)


▶ 오늘의 오류 발생 및 해결 과정

	1. 카프카와 로그스태시 연결 오류
		→ 서로 다른 컨테이너끼리의 연결오류, 같은 네트워크 상에 존재하지 않아서 발생하는 것 같아 보임 ( 아직 정확한 오류 파악 못함 )	
	
	2. hadoop 설치 시 각 노드마다 yaml을 설정해줘야(pod, service 등) 동작하는 것으로 보임
		→ 조금 더 알아보고 
		
**2022 / 05 / 10**

- Kafka 도커 이미지 만들기
- Kafka 이미지를 사용해서 디플로이먼트 생성
- Kafka & Logstash 연결
- hdfs-base 도커 파일 작성 후 도커 이미지 생성, 도커 허브에 배포
- hadoop namenode / datanode / journalnode yaml 파일을 프로젝트에 맞게 수정, 작성

▶ 오늘의 오류 발생 및 해결 과정

	1. 로그스태시 실행 시 카프카 컨테이너에 접근할 수 없다는 오류
		→ 카프카안에 conf 파일들에 localhost:9092 로 되어있는 것들을 카프카 컨테이너의 ip 와 부여받은 포트로 변경시도 172.30.1.129:30901

	2. vi /etc/hosts 안에 카프카 파드 ip와 워커노드 ip 중 어떤 것을 사용해야하는지 모르겠다.
		→ 172번대 ip ( 워커노드 ip ) 아닌 198번대 ip ( 컨테이너 ip )로 logstash.conf 파일을 실행시키니깐 오류의 반복은 사라짐 (  Error connecting to node kafka-797db9cb6f-tsjxt:9092 (id: 0 rack: null) )
		→ 하지만, disconnect 문제가 발생함 (  Bootstrap broker 192.168.182.14:9092 (id: -1 rack: null) disconnected )
	3. 도커파일, yaml파일을 프로젝트에 맞게 수정하는 과정 중 문제
		→ 도커파일 : 참고 자료에서는 debian 기반이라 프로젝트에 맞게(ubuntu) 수정
		→ yaml : service 방식이 달라서 프로젝트에 맞게(metallb) 수정하고 Pod 대신 Deployment로 수정

**2022 / 05 / 11**

-  Kafka & Logstash 연결
- Github에 프로그램.yaml 파일들 올리기

▶ 오늘의 오류 발생 및 해결 과정 

	1. 오늘의 발견

		1) 일단 데이터 스트링을 보내는 코드를 통해 파이썬에서 카프카 브로커를 이용하는 법을 테스트하여 파이썬에서 컨테이너안에 카프카 브로커에게 전송이 되는지 확인  
			→ 파이썬에 존재하는 본체 컴퓨터에서 ping 테스트를 통해서 172.30.1.129 ( 워커노드 ip ) , 192.168.182.15 ( 컨테이너 ip ) , 172.30.1.182 ( 서비스 ip ) 테스트
			→ 결과는 172.30.1.129 워커노드 ip 로만 통신이 가능했다. 따라서 파이썬에서 카프카로 보낼때는 워커노드 ip 로 전송을 해야한다.
			→ 그런데 카프카 자체에서는 192.168.182.15 ip로 통신이 되게끔 설정이 되어있다.
			→ 그리하여 카프카 내 conf 파일들의 localhost:9092 들을 모두 172.30.129:31194로 바꾸어보려고 한다.   

	2. 로그스태시에서 카프카 브로커를 통해 파이썬 컨슈머에게 메시지 전달
			→ vi /opt/kafka/config/server.properties 에서 추가 내용 중 ip 관련 오류 발생  
			   쿠버네티스 내에서 컨테이너의 이름을 찾아서 접속은 가능하나 다른 곳에서는 이 컨테이너 이름을 찾아서 접속할 수 없기때문에 아이피 주소를 직접 입력  

			→ vi /opt/kafka/config/server.properties
			  	kubectl apply -f kafka.yaml
			  	kubectl expose deploy kafka --type LoadBalancer --name kafka


		  	  vi /etc/hosts ( 브로커 안에는 컨슈머 프로듀서 전부 추가 )
				192.168.182.19:9092 kafka-646c48c49b-69tvs          → broker
				172.30.1.181 DESKTOP-KAV6C2V                        → consumer
				192.168.235.134:5044 logstash-575c848476-cqjv9     → producer


			   vi /etc/hosts ( 프로듀서랑 컨슈머 안에는 브로커만 추가 )
				192.168.182.18:9092 kafka-646c48c49b-69tvs          → broker


			   vi /opt/kafka/config/server.properties
				listener.security.protocol.map=EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT
				listeners=INTERNAL://kafka-646c48c49b-69tvs:9092,EXTERNAL://0.0.0.0:9093
				advertised.listeners=INTERNAL://kafka-646c48c49b-69tvs:9092,EXTERNAL://kafka-646c48c49b-69tvs:31600
				inter.broker.listener.name=INTERNAL

			    ./bin/zookeeper-server-start.sh ./config/zookeeper.properties
			    ./bin/kafka-server-start.sh ./config/server.properties
			    
			    
![image](https://user-images.githubusercontent.com/97823665/167807204-3d9cbcc2-aed5-4ba2-b550-98e2ab623ed6.png)  
→ 다음과 같이 logstash-sample.conf 파일에서 카프카에 topic "test3"을 만들게 지정하여 실행을 시키면

![image](https://user-images.githubusercontent.com/97823665/167808352-59fbb2af-34ba-46c3-8007-572fdf5a1017.png)  
→ Kafka에 test3이 만들어졌다는 log들이 뜨고

![image](https://user-images.githubusercontent.com/97823665/167808604-2c7f5512-07ec-4c0b-bede-5fbd98753ce4.png)  
→ test3 topic이 새롭게 생성된 것을 확인할 수 있다.

**2022 / 05 / 12**

- 외부 컴퓨터와 Kafka & Logstash 연결 후 데이터 흐름 테스트
- hadoop 노드 변경(namenode, datanode, nodemanager, resourcemanager, historyserver, spark)
- 각 노드를 도커파일로 작성 후 도커 이미지 생성 / 도커 허브에 배포

▶ 오늘의 오류 발생 및 해결 과정

	1. 카프카에 생성된 토픽을 삭제하기 위해서 zookeeper 쉘에 접속하여 명령어를 사용하는데 오류발생
		→ rmr /brokers/topics/토픽이름 : command not found 라고 해서 검색을 통해 다른 명령어 실행  
		→ deleteall /brokers/topics/토픽이름 : delete 명령어만 사용하면 node not empty 라고 나오기때문에 deleteall 명령어 사용  

	2. 카프카 broker /etc/hosts/ 파일에 카프카 브로커의 ip를 외부에서 접속할 수 있는 ip로 변경 필요  
		→ kafka에서 vi /etc/hosts 로 172.30.1.129:31600 kafka-646c48c49b-69tvs 추가  

	3. /etc/hosts에서 ip를 master:port 로 변경  
		→ master:port 로 지정해주면 master에서 알아서 워커노드로 접속하도록 유도  

	4. server.properties 에서 EXTERNAL://172.30.1.222:31256(9093의 외부포트)  
		→ 파이썬에서 카프카 접근하는 것은 외부에서 접근하는 것이기 때문에 포트를 9093 포트로 바꿔주어야 한다.  

	5. zookeeper 가 계속 카프카를 잃어버린다..  
		→ 왜 ㅜㅜ 잃어버리냐..  
		
	6. In the log4j.properties, change the value of two fields from info to debug.
		→ log4j.logger.kafka=INFO to log4j.logger.kafka=DEBUG


: 파이썬 producer가 kafka consumer에게 보내는건 성공! kafka produer가 파이썬 consumer에게 보내는건 실패!   
![image](https://user-images.githubusercontent.com/97823665/168031883-d10172fc-134e-41a1-95c7-57e3e10aceb2.png)

	7. 도커파일 작성 후 이미지 생성 중 오류가 발생
	1) WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
		→ apt를 apt-get으로 변경하여 해결
	2) gpg: no ultimately trusted keys found
		→ add-apt-repository ppa:nilarimogard/webupd8
		→ apt-get update
		→ apt-get install launchpad-getkeys
		→ launchpad-getkeys 입력하여 

**2022 / 05 / 13**

- 각 노드들의 yaml파일 내용을 프로젝트에 맞게 수정  (namenode, datanode, nodemanager, resourcemanager, historyserver, spark)
- 도커 이미지를 기반으로 k8s에 Deployment 생성
- 외부컴퓨터와 Kafka & Logstash 연결 후 통신 테스트 완료
- Kafka & Logstash & Spark 연결  
	→ 참고 사이트   
	https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html      
	https://willbesoon.tistory.com/268      
   	https://www.wenyanet.com/opensource/ko/6121b1558d252d1df0132ff7.html    
  
▶ 오늘의 오류 발생 및 해결 과정

	1. 도커 허브에 push 후 불필요한 도커 이미지를 삭제 하려는데 삭제가 되지않음
		→ 이미지 강제삭제 명령어 사용 : docker rmi $(docker images -q) -f
		
	2. metallb로 외부와 연결 / service.yaml을 사용하여 외부와 연결 동시에 가능할 지 구글링 (진행중)
	 
	3. Kafka producer가 Kafka consumer에게 값을 2개이상 보내면 NotLeaderOrFollowerException 다음과 같은 오류 발생   
		→ /config/server.properties  에서 auto.leader.rebalance.enable=true 명령어 추가했지만 실패  
		→ 토픽 생성 시 파티션을 1개만 설정하니 해결된다는 정보 ( 출처 : https://www.facebook.com/groups/kafka.kru/posts/1296021857505801/ )  
		→ 근데 이상한 점을 발견.   
		    1) 로그스태시가 producer고, kafka가 consumer일때 생성한 topic test4에서는 카프카 내부 통신이 가능  
		    2) 새로 topic을 생성해서 카프카 내부통신을 하면 오류가 나는 줄 알았는데 다시 된다.  
		       → 일전에 test1 ~ test4까지 토픽을 생성했다가 지웠던 적이 있는데 .. 아마 그 이름들로 topic을 생성하면 충돌이 일어나서 오류가 나는 것은 아닐까 하는 자그마한 생각이었다.  
		       
		       


#### < 3주차 >

![image](https://user-images.githubusercontent.com/97823665/168728855-6f381f8f-c417-4fa9-91de-7e278d903c74.png)  
→ 그림 출처 : https://github.com/hrchlhck/k8s-bigdata

- 하둡 namenode, datanode, historyserver, resourcemanager ( 진행중 )
- 하둡 namenode, datanode, historyserver, resourcemanager 연결 ( 포기 )
- 카프카 & 로그스태시 & 스파크 연결 ( 진행중 )
- 젠킨스 구축 및 Git연동(IN 쿠버)  ( 완료 )
- Django 이미지 생성 및 POD 생성 ( 완료 )
- Django pod 브라우저 외부노출 테스트(IN 쿠버) ( 완료 ) 
- Django 배포 - APACHE로 결정(IN 쿠버) ( 완료 )

**2022 / 05 / 16**

- Hadoop namenode, datanode, historyserver, resourcemanager 각각 구축 후 테스트
- 프로젝트에 맞게 도커파일, 도커이미지 수정


▶ 오늘의 오류 발생 및 해결 과정

	1. ssh 설치 후 비밀번호 입력 오류
		→  ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa 명령어 실행으로 해결
	
	2. namenode 실행 시 JAVA_HOME 설정 오류
		→ vi .bashrc 에서 export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64  추가

	3. error: resource mapping not found for name: "namenode" namespace: "default" from "namenode.yaml": no matches for kind "Deployment" in version "v1"
ensure CRDs are installed first	
		→ v1를 apps/v1로 변경 (서비스는 v1 그대로)
		
	4. error: error validating "namenode.yaml": error validating data: [ValidationError(Deployment.spec): unknown field "containers" in io.k8s.api.apps.v1.DeploymentSpec, ValidationError(Deployment.spec): unknown field "volumes" in io.k8s.api.apps.v1.DeploymentSpec]
		→ yaml파일 안에 spec: 빠져있었음, 넣은 다음 yaml문법에 맞게 들여쓰기 하기
		
	5. yaml 문법에 맞는 지 미리 확인이 필요	
		→ http://www.yamllint.com/ yaml 검사기 활용



**2022 / 05 / 17**

- hadoop 각 노드들끼리 ping, ssh 통신 연결
- Hadoop namenode, datanode, historyserver, resourcemanager 하나의 클러스터로 연결
- 쿠버네티스 젠킨스 구축 후 Git과 연동


▶ 오늘의 오류 발생 및 해결 과정

	1. 22번 포트 연결 거절, 권한 없음 등 오류가 발생
		→  openssh server / client 재설치
		
	2. ssh 0.0.0.0 connection refused 오류
		→  vi /etc/hosts.allow ssh:ALL:allow\sshd:ALL:allow 명령어 추가
		→  service ssh start

	3. apt install vim 오류 Unable to locate package vim 
		→ apt-get update 후 apt-get install vim 하면 올

	4. kubernetes에 jenkins 설치 후 웹 접속하여 계정 생성 후 재시작하면 다시 초기화 상태로 돌아가는 오류
		→ 재시작을 안하는 방향으로 진행

		
		


**2022 / 05 / 18**

- hadoop 각 노드들끼리 ping, ssh 통신 연결 진행중
- Hadoop namenode, datanode, historyserver, resourcemanager 하나의 클러스터로 연결
- 쿠버네티스 젠킨스 구축 후 Git과 연동
- Git 에서 PUSH 하면 Jenkins에 업데이트 되는지 테스트 확인
- 쿠버네티스에 파이썬 장고 구축


▶ 오늘의 오류 발생 및 해결 과정

	1. 22번 포트 연결 거절, 권한 없음 등 오류가 계속 발생
		→  vi /etc/ssh/sshd_config
		→  apt-get install ufw
		→  vi /etc/hosts.allow
		→  passwd 설정
	
	2. 젠킨스에서 프로젝트 생성 후 빌드 실패 오류 
		→  깃에 실제로 존재하는 브랜치 이름으로 빌드를 작성해주어야 한다
	
	3. jenkins we couldn’t deliver this payload: failed to connect to host
		→ ngrok http 172.30.1.222:30000 등록
	
	4. Your account is limited to 1 simultaneous ngrok client session. ERROR: Active ngrok agent sessions in region 'jp':
		→ tskill /A ngrok




**2022 / 05 / 19**

- hadoop 각 노드들끼리 ping, ssh 통신 연결 (완료)
- 쿠버네티스 클러스터에 jenkins 설치 및 설정

▶ 오늘의 오류 발생 및 해결 과정

	1. 포트 타임아웃 등 오류가 계속 발생
		→  ssh-copy-id 명령어가 안되서 키를 직접 복사해서 넣기
	
	2. 젠킨스 설치 후 플러그인 파일들이 pv, pvc에 저장 
		→  pv와 pvc를 따로 생성하지 않고 jenkins 저장소를 서버 디렉토리에 mount시키는 방법으로 진행  
	  	
	3. 마스터노드에서 사용자 생성을 해서 실행을 시키면 Contrl-Plane Node에 Pod를 못 올리도록 설정되어 있기 때문에 오류 발생  
		→  taint 설정을 해제시켜주는 작업이 필요  

	4. var/log에 파일들을 touch 하려고 하는데 permission denied 오류 발생
		→ 컨테이너를 관리자 권한으로 실행


**2022 / 05 / 20**

- 젠킨스로 쿠버네티스 배포 ( Dockerfile & Jenkinsfile 어떤 내용 들어가야 하는지 공부 )
- hadoop,spark 각 노드들 연결시도

▶ 오늘의 질문

	1. .env 파일이 쿠버네티스 노드 안에 존재해야하는가
		→  /home/k8s/django 안에 존재해야한다

	2. 도커 이미지 생성할 때마다 태그 숫자가 변경 되어야 하는거 아닌가
		→  이미지 태그는 동일한 태그에 계속 덮어쓰기 해도 상관이 없다
		
	3. datanode가 갑자기 죽어서 설정 파일 등이 날아가는 사건이 발생
		→ dockerfile, dockerimage, yaml 모두 수정해야하는 필요성을 느낌




**2022 / 05 / 21**

- 젠킨스로 쿠버네티스 배포
- dockerfile 작성법 숙지

▶ 오늘의 오류 발생 및 해결 과정

	1. 워커3에서 docker status failed
		→  worker3에 설치된 docker.socket 실행파일이 directory 형태로 되어있어서 생긴 문제
		→ rm -rf docker.socket 폴더를 지우고 systemctl restart docker하면 제대로 실행

	2. Dockerfile 에서 python에 requirements.txt를 설치해야하는데 연결오류 발생
		→ 아까 docker 오류 해결하려고 /etc/docker/daemon.json 파일을 생성
		→ 해당 파일을 삭제하고 systemctl restart docker 후 진행하면 오류 해결



**2022 / 05 / 22**

- Dockerfile & Jenkinsfile & requirements.txt 패키지 버전 변경
- Autoscailing yaml 파일 공부
- dockerfile / 쉘스크립트(.sh) 문법 숙지, 테스트

▶ 오늘의 질문

	1. verymarket-main.yaml 파일에 이미지를 어떤거 땡겨와야 하나?
		→  nginx 인지 아니면 내가 만든 runserver 할 수 있는 이미지 인지

	2. 장고 프로젝트를 쿠버에 올려야하는건지? 
		→ 젠킨스를 이용항 쿠버네티스 배포인데.. 정확하게 순서가 어떻게 되는건지
		
	3. 테스트용 dockerfile을 생성했지만 설정파일이 제대로 되지않음
		→ 다시해보기..







#### < 4주차 >

- 젠킨스를 이용한 쿠버네티스 배포 CI/CD ( 완료 )
- 하둡 + 스파크 + 제플린 도커이미지를 통한 구축 ( 완료 ) 
- 하둡 + 스파크 + 제플린 연결 ( 완료 )
- 카프카 + 스파크 연결 후 데이터 이동 테스트
- 웹 사이트 로그데이터 → 카프카 → 스파크 → 제플린 이동 테스트 
- API 데이터를 ELK 수집, 저장, 분석, 시각화 데이터 이동 테스트 ( 완료 )
- API 데이터를 업데이트하며 ELK 수집, 저장, 분석, 시각화 ( 완료 )

**2022 / 05 / 23**

- Jenkins CI/CD 배포 완료
- 판매자 사이트 장고를 위한 Jenkins2 구축 및 연동


▶ 오늘의 오류 및 해결과정

	1. 프로젝트 안에 존재하는 deployment.yaml 파일이 불러올 이미지는 verymarket-main:0.2이미지 여야한다
		→  nginx 이미지는 필요없는거였고, 내가 gunicorn을 통해서 불러온 장고이미지를 가져와야했던 것이다.


**2022 / 05 / 24**

- 판매자 사이트를 위한 jenkins를 새로 구축하지 않고 기존 젠킨스에 또다른 pipeline 추가
- hadoop-spark-zeppelin 설치 / 연결 완료

▶ 오늘의 오류 및 해결과정

	1. verymarket-main:0.1 이미지 불러오는데 python-dev 설치 오류
		→  도커파일에 RUN apt-get update 명령어 실행

	2. ClusterRoleBinding 생성 오류
		→ apiVersion: rbac.authorization.k8s.io/v1beta1 을 apiVersion: rbac.authorization.k8s.io/v1으로 변경
		
	3. 데이터 조회 오류
		→ 날짜 설정 오류


**2022 / 05 / 25**

- 오픈 API 데이터를 로그스태시로 수집 후 엘라스틱에 저장 테스트 
- 판매자사이트 장고 DB 연결 후 젠킨스 배포
- 오픈 API 데이터(JSON, 매 시간마다 업데이트)를 python으로 불러오기
- 매 시간마다 알아서 JSON 파일로 저장하는 코드 작성중

▶ 오늘의 오류 및 해결과정

	1. python producer가 kafka 브로커에게 json 파일 보내기 decode error
		→  json파일을 producer.send를 했을 때 json파일로 다시 load 하려고 해서 오류 발생

	2. kibana에서 elasticsearch ip 인식오류 ( 옛날 주소만 인식하고 새로운 ip주소를 인식을 못한다 )
		→  elasticsearch와 kibana pod에 접속해서 service들을 모두 재시작시키면 된다	
	
	3. elasticsearch에서 mapping 할때 numeric data type 지정해주고 싶어서 type : int 로 작성 시 오류
		→ numeric data type 지정해줄 때 int가 아니고 integer로 지정
 
**2022 / 05 / 26**

- 오픈 API 데이터 엘라스틱에 파일 형식 깨져서 저장되는 오류 해결
- 매일매일 데이터 업데이트 되도록 파이썬 코드 수정
- 업데이트 되는 데이터 지속적으로 수집 저장 할 수 있는 방법 찾기

▶ 오늘의 오류 및 해결과정

	1. 한글깨짐 오류, 키값 인식 오류
		→  원인을 모름
	
	


**2022 / 05 / 27**

- Django 로그 데이터 수집을 위한 코드 작성, 프로젝트에 맞게 수정
- Django yaml파일 수정
- 오픈 API xml TO json 방법 찾아서 한글깨짐과 키값인식 오류 해결

▶ 오늘의 오류 및 해결과정

	1. Filter, 수집할 로그 Level 수정
		→ Django settings.py에서 DEBUG 값에 맞추기위해 Filter에서 DEBUG=False를 삭제
		→ 수집 로그 Level을 Info 에서 DEBUG로 수정 (일단 모든 로그를 수집할 목적)
		
	2. 테스트용으로 로그 데이터를 수집하려했으나 메모리 부족 등으로 exit code 137이 뜨면서 파드가 강제 삭제됨
		→ Django yaml파일을 수정
		→ container에서 resources를 추가하여 메모리 부족 해결중
		
		
**2022 / 05 / 28**

- Django yaml파일 수정
- 오픈 API str TO json 방법 찾아서 한글깨짐과 키값인식 오류 해결

▶ 오늘의 오류 및 해결과정

	1. 백엔드 팀과 협업하여 GitHub에서 Django yaml파일 수정, Jenkins를 통해 k8s에서 새로 Django Deployment가 만들어졌으나 오류로 인해 만들어지지않음
		→ Django yaml파일을 다시 수정
		
		
**2022 / 05 / 29**

- Django yaml파일 수정하여 Pods 가동 시도
- 오픈API 날마다 업데이트 되도록 데이터 조회코드 생성 & 엘라스틱에 저장
- 기상청API 날마다 업데이트 되도록 데이터 조회코드 생성 & 엘라스틱에 저장
- 뉴스 크롤링 파이썬 코드 생성

▶ 오늘의 오류 및 해결과정

	1. json인척하는 str 문장을 json형식으로 카프카에 들어가게끔 했지만 한글깨짐 과 키값 인식 오류
		→ 결국 해결하지 못하고 xml 데이터 조회 코드까지만 작성.. 하는줄 알았는데

	2. input kafka에서 codec => json{}
		→ json {} 안에 아무것도 넣었으면 안되는 거였다 결국 성공
		
		
		
**2022 / 05 / 30**

- Django yaml파일, logging 수정하여 Pod 가동
- 


▶ 오늘의 오류 및 해결과정

	1. Pod는 가동되었으나 배포한 사이트의 로그가 수집되지않음
		→ logging 코드 다시 수정
		→ 'filename'을 BASE_DIR / 'logs/verymarket.log', ---> os.path.join(BASE_DIR, 'logs') + "/log", 로 변경
		→ 'format'을 '{levelname} {asctime} {module} {message}', ---> '%(asctime)s [%(levelname)s] %(name)s: %(message)s' 로 변경
		→ 'level'을 DEBUG ---> INFO 로 변경
		
	2. localhost에서 로그 데이터는 수집이 됨
		→ k8s 환경에서 로그 데이터를 받게끔 테스트 중
		
		
		
		
**2022 / 05 / 31**

- Django log data 일부를 Python으로 전처리
- ㅇㅇ


▶ 오늘의 오류 및 해결과정

	1. Log data 중 필요한 부분만 남기고 지워야하는데 지워지지않음
		→ 2022-05-31 16:53:47 [INFO] django.server: 주문번호:30 유저번호:1 품목:망고 수량:1
		→ 공백을 ,로 치환 / ,를 기준으로 split / 각 data를 변수로 지정 / 필요없는 변수를 삭제
		→ 날짜 : [0], 시간 : [1], 로그 레벨 : [2], 로그가 발생하는 곳 : [3], 주문번호 : [4], 유저번호 : [5], 품목 :  [6], 수량 : [7]
		→ [2], [3]은 삭제 / [4], [5], [6], [7]은 한글 삭제
		→ [2]은 re.sub, replace 등을 사용해도 지워지지않음
		
	2. ㅇㅇ
		→
		
		
		
		
**2022 / 06 / 02**

- Python으로 전처리 과정 완료
- Python으로 전처리 한 Log data를 Kafka로 보내기
- Zeppelin 사용방법 숙지


▶ 오늘의 오류 및 해결과정

	1. python으로 전처리 해결
		→ [2], [3]을 아예 변수로 설정하지않음
	
	2. 간단한 코드를 작성하였으나 Interpreter process is not running 오류가 뜸
		→ spark interpreter에서 설정부분 수정


