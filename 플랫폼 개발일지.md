# 플랫폼 개발일지


#### < 1주차 >

- 쿠버네티스 컨테이너디 환경 세팅 ( 완료 )
- ELK 설치 ( 완료 )
- 파이프라인 시나리오 작성 ( 진행중 ) 
- 카프카 올리기 ( 완료 )
- 하둡 이미지 빌드하기 ( 완료 )
- 하둡 이미지 위에 스파크 올려서 이미지 빌드하기 ( 완료 )
- 하둡 + 스파크 이미지 올리기 ( 완료 )
- 하둡 + 스파크 이미지 위에 제플린 올려서 이미지 빌드하기 ( 완료 )
- 하둡 + 스파크 + 제플린 이미지 사용해서 디플로이먼트 올리기 ( 완료 )
- 쿠버네티스 디플로이먼트 포트포워딩 고려 ( 추후과제 )
- 프로그램 개발환경 테스트 ( 진행중 )
- 프로그램들 모두 연결하기 ( 추후과제 )
- 쿠버네티스 컨테이너 기능들 추가해보기 ( 보류 )



**2022 / 05 / 03**

- Containerd 설치
- Kubernetes 설치

	master node 1대  
	worker node 5대

	master 172.30.1.222  
	worker1 172.30.1.2  
	worker2 172.30.1.62  
	worker3 172.30.1.129  
	worker4 172.30.1.144  
	worker5 172.30.1.194  

▶ 오늘의 오류 발생 및 해결 과정
	
	1. calico node 0/1 문제 발생
		: 방화벽 문제인줄 알고 방화벽 해제 했지만 계속 같은 문제 발생
		  → 가상머신 Network를 Bridge로 설정하지 않고 NAT로 한 것이 원인

**2022 / 05 / 04**

- Elasticsearch 설치
- Logstash 설치
- Kibana 설치
- Spark 설치


▶ 오늘의 오류 발생 및 해결 과정

	1. apt install docker.io 설치 오류
		→ apt install containerd 실행하고 다시 apt install docker.io 진행하기

**2022 / 05 / 05**

- Ubuntu 이미지 위에 Hadoop 설치

▶ 오늘의 오류 발생 및 해결 과정

	1. ssh 연결 오류
		→ vi /etc/hosts.allow 파일에 ssh:ALL:allow\sshd:ALL:allow 추가 후
                             service ssh restart 명령어 실행

	2. 도커 이미지 push 시 requeste denied 오류 발생
		→ docker login이 안되어 있거나 이미지 username과 docker hub id가 일치하지 않을 때 발생

**2022 / 05 / 06**

- Ubuntu 이미지 위에 kafka 설치

▶ 오늘의 오류 발생 및 해결 과정

	1. /opt/kafka/bin/zookeeper-server-start.sh config/zookeeper.properties 실행이 안됨
		→ producer와 consumer가 아직 준비 되지 않아서 설치로 끝냄

**2022 / 05 / 07**

- Ubuntu + Hadoop 이미지 위에 Spark 설치

▶ 오늘의 오류 발생 및 해결 과정

	1. bash : start command not found
		→ .bashrc 에 환경변수 설정을 안넣어줘서 발생하는 오류

**2022 / 05 / 08**

- Ubuntu + Hadoop + Spark 이미지 위에 Zepplin 설치

▶ 오늘의 오류 발생 및 해결 과정

	1. wget으로 zeppelin 파일을 다운받을 때 root 디렉토리에 저장하지 않아서 경로 설정에 충돌 발생 

	2. 컨테이너 자체를 접속할 때 포트포워딩을 하고 접속을 하면 되는데 디플로이먼트로 생성 후 포트포워딩을 어떻게 해야할지 생각해보기  





#### < 2주차 >

- 하둡&스파크 / 제플린 컨테이너 분리 ( 진행중 )
- hadoop, spark, zeppelin on k8s 구현 ( 진행중 )
- 카프카 & 로그스태시 연결 ( 진행중 )
- 뉴스 크롤링 데이터를 통한 프로그램 간 데이터 이동 테스트 
- helm 사용하기 (진행중)


**2022 / 05 / 09**

- hadoop, spark clustering 구현 진행중
- Kafka & Logstash 연결
- helm을 프로젝트 환경에 맞게 사용하기 (불필요한 chart 삭제 등, 차지하는 데이터가 많으면 사용 보류)


▶ 오늘의 오류 발생 및 해결 과정

	1. 카프카와 로그스태시 연결 오류
		→ 서로 다른 컨테이너끼리의 연결오류, 같은 네트워크 상에 존재하지 않아서 발생하는 것 같아 보임 ( 아직 정확한 오류 파악 못함 )	
	
	2. hadoop 설치 시 각 노드마다 yaml을 설정해줘야(pod, service 등) 동작하는 것으로 보임
		→ 조금 더 알아보고 
		
**2022 / 05 / 10**

- Kafka 도커 이미지 만들기
- Kafka 이미지를 사용해서 디플로이먼트 생성
- Kafka & Logstash 연결
- hdfs-base 도커 파일 작성 후 도커 이미지 생성, 도커 허브에 배포
- hadoop namenode / datanode / journalnode yaml 파일을 프로젝트에 맞게 수정, 작성

▶ 오늘의 오류 발생 및 해결 과정

	1. 로그스태시 실행 시 카프카 컨테이너에 접근할 수 없다는 오류
		→ 카프카안에 conf 파일들에 localhost:9092 로 되어있는 것들을 카프카 컨테이너의 ip 와 부여받은 포트로 변경시도 172.30.1.129:30901

	2. vi /etc/hosts 안에 카프카 파드 ip와 워커노드 ip 중 어떤 것을 사용해야하는지 모르겠다.
		→ 172번대 ip ( 워커노드 ip ) 아닌 198번대 ip ( 컨테이너 ip )로 logstash.conf 파일을 실행시키니깐 오류의 반복은 사라짐 (  Error connecting to node kafka-797db9cb6f-tsjxt:9092 (id: 0 rack: null) )
		→ 하지만, disconnect 문제가 발생함 (  Bootstrap broker 192.168.182.14:9092 (id: -1 rack: null) disconnected )
	3. 도커파일, yaml파일을 프로젝트에 맞게 수정하는 과정 중 문제
		→ 도커파일 : 참고 자료에서는 debian 기반이라 프로젝트에 맞게(ubuntu) 수정
		→ yaml : service 방식이 달라서 프로젝트에 맞게(metallb) 수정하고 Pod 대신 Deployment로 수정

**2022 / 05 / 11**

-  Kafka & Logstash 연결
- Github에 프로그램.yaml 파일들 올리기

▶ 오늘의 오류 발생 및 해결 과정 

	1. 오늘의 발견

		1) 일단 데이터 스트링을 보내는 코드를 통해 파이썬에서 카프카 브로커를 이용하는 법을 테스트하여 파이썬에서 컨테이너안에 카프카 브로커에게 전송이 되는지 확인  
			→ 파이썬에 존재하는 본체 컴퓨터에서 ping 테스트를 통해서 172.30.1.129 ( 워커노드 ip ) , 192.168.182.15 ( 컨테이너 ip ) , 172.30.1.182 ( 서비스 ip ) 테스트
			→ 결과는 172.30.1.129 워커노드 ip 로만 통신이 가능했다. 따라서 파이썬에서 카프카로 보낼때는 워커노드 ip 로 전송을 해야한다.
			→ 그런데 카프카 자체에서는 192.168.182.15 ip로 통신이 되게끔 설정이 되어있다.
			→ 그리하여 카프카 내 conf 파일들의 localhost:9092 들을 모두 172.30.129:31194로 바꾸어보려고 한다.   

	2. 로그스태시에서 카프카 브로커를 통해 파이썬 컨슈머에게 메시지 전달
			→ vi /opt/kafka/config/server.properties 에서 추가 내용 중 ip 관련 오류 발생  
			   쿠버네티스 내에서 컨테이너의 이름을 찾아서 접속은 가능하나 다른 곳에서는 이 컨테이너 이름을 찾아서 접속할 수 없기때문에 아이피 주소를 직접 입력  

			→ vi /opt/kafka/config/server.properties
			  	kubectl apply -f kafka.yaml
			  	kubectl expose deploy kafka --type LoadBalancer --name kafka


		  	  vi /etc/hosts ( 브로커 안에는 컨슈머 프로듀서 전부 추가 )
				192.168.182.19:9092 kafka-646c48c49b-69tvs          → broker
				172.30.1.181 DESKTOP-KAV6C2V                        → consumer
				192.168.235.134:5044 logstash-575c848476-cqjv9     → producer


			   vi /etc/hosts ( 프로듀서랑 컨슈머 안에는 브로커만 추가 )
				192.168.182.18:9092 kafka-646c48c49b-69tvs          → broker


			   vi /opt/kafka/config/server.properties
				listener.security.protocol.map=EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT
				listeners=INTERNAL://kafka-646c48c49b-69tvs:9092,EXTERNAL://0.0.0.0:9093
				advertised.listeners=INTERNAL://kafka-646c48c49b-69tvs:9092,EXTERNAL://kafka-646c48c49b-69tvs:31600
				inter.broker.listener.name=INTERNAL

			    ./bin/zookeeper-server-start.sh ./config/zookeeper.properties
			    ./bin/kafka-server-start.sh ./config/server.properties
			    
			    
![image](https://user-images.githubusercontent.com/97823665/167807204-3d9cbcc2-aed5-4ba2-b550-98e2ab623ed6.png)  
→ 다음과 같이 logstash-sample.conf 파일에서 카프카에 topic "test3"을 만들게 지정하여 실행을 시키면

![image](https://user-images.githubusercontent.com/97823665/167808352-59fbb2af-34ba-46c3-8007-572fdf5a1017.png)  
→ Kafka에 test3이 만들어졌다는 log들이 뜨고

![image](https://user-images.githubusercontent.com/97823665/167808604-2c7f5512-07ec-4c0b-bede-5fbd98753ce4.png)  
→ test3 topic이 새롭게 생성된 것을 확인할 수 있다.

**2022 / 05 / 12**

- 외부 컴퓨터와 Kafka & Logstash 연결 후 데이터 흐름 테스트
- hadoop 노드 변경(namenode, datanode, nodemanager, resourcemanager, historyserver, spark)
- 각 노드를 도커파일로 작성 후 도커 이미지 생성 / 도커 허브에 배포

▶ 오늘의 오류 발생 및 해결 과정

	1. 카프카에 생성된 토픽을 삭제하기 위해서 zookeeper 쉘에 접속하여 명령어를 사용하는데 오류발생
		→ rmr /brokers/topics/토픽이름 : command not found 라고 해서 검색을 통해 다른 명령어 실행  
		→ deleteall /brokers/topics/토픽이름 : delete 명령어만 사용하면 node not empty 라고 나오기때문에 deleteall 명령어 사용  

	2. 카프카 broker /etc/hosts/ 파일에 카프카 브로커의 ip를 외부에서 접속할 수 있는 ip로 변경 필요  
		→ kafka에서 vi /etc/hosts 로 172.30.1.129:31600 kafka-646c48c49b-69tvs 추가  

	3. /etc/hosts에서 ip를 master:port 로 변경  
		→ master:port 로 지정해주면 master에서 알아서 워커노드로 접속하도록 유도  

	4. server.properties 에서 EXTERNAL://172.30.1.222:31256(9093의 외부포트)  
		→ 파이썬에서 카프카 접근하는 것은 외부에서 접근하는 것이기 때문에 포트를 9093 포트로 바꿔주어야 한다.  

	5. zookeeper 가 계속 카프카를 잃어버린다..  
		→ 왜 ㅜㅜ 잃어버리냐..  
		
	6. In the log4j.properties, change the value of two fields from info to debug.
		→ log4j.logger.kafka=INFO to log4j.logger.kafka=DEBUG


: 파이썬 producer가 kafka consumer에게 보내는건 성공! kafka produer가 파이썬 consumer에게 보내는건 실패!   
![image](https://user-images.githubusercontent.com/97823665/168031883-d10172fc-134e-41a1-95c7-57e3e10aceb2.png)

	7. 도커파일 작성 후 이미지 생성 중 오류가 발생
	1) WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
		→ apt를 apt-get으로 변경하여 해결
	2) gpg: no ultimately trusted keys found
		→ add-apt-repository ppa:nilarimogard/webupd8
		→ apt-get update
		→ apt-get install launchpad-getkeys
		→ launchpad-getkeys 입력하여 

**2022 / 05 / 13**

- 각 노드들의 yaml파일 내용을 프로젝트에 맞게 수정  (namenode, datanode, nodemanager, resourcemanager, historyserver, spark)
- 도커 이미지를 기반으로 k8s에 Deployment 생성
- 외부컴퓨터와 Kafka & Logstash 연결 후 통신 테스트 완료
- Kafka & Logstash & Spark 연결  
	→ 참고 사이트   
	https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html      
	https://willbesoon.tistory.com/268      
   	https://www.wenyanet.com/opensource/ko/6121b1558d252d1df0132ff7.html    
  
▶ 오늘의 오류 발생 및 해결 과정

	1. 도커 허브에 push 후 불필요한 도커 이미지를 삭제 하려는데 삭제가 되지않음
		→ 이미지 강제삭제 명령어 사용 : docker rmi $(docker images -q) -f
		
	2. metallb로 외부와 연결 / service.yaml을 사용하여 외부와 연결 동시에 가능할 지 구글링 (진행중)
	 
	3. Kafka producer가 Kafka consumer에게 값을 2개이상 보내면 NotLeaderOrFollowerException 다음과 같은 오류 발생   
		→ /config/server.properties  에서 auto.leader.rebalance.enable=true 명령어 추가했지만 실패  
		→ 토픽 생성 시 파티션을 1개만 설정하니 해결된다는 정보 ( 출처 : https://www.facebook.com/groups/kafka.kru/posts/1296021857505801/ )  
		→ 근데 이상한 점을 발견.   
		    1) 로그스태시가 producer고, kafka가 consumer일때 생성한 topic test4에서는 카프카 내부 통신이 가능  
		    2) 새로 topic을 생성해서 카프카 내부통신을 하면 오류가 나는 줄 알았는데 다시 된다.  
		       → 일전에 test1 ~ test4까지 토픽을 생성했다가 지웠던 적이 있는데 .. 아마 그 이름들로 topic을 생성하면 충돌이 일어나서 오류가 나는 것은 아닐까 하는 자그마한 생각이었다.  
		       
		       


#### < 3주차 >

![image](https://user-images.githubusercontent.com/97823665/168728855-6f381f8f-c417-4fa9-91de-7e278d903c74.png)  
→ 그림 출처 : https://github.com/hrchlhck/k8s-bigdata

- 하둡 namenode, datanode, historyserver, resourcemanager ( 진행중 )
- 하둡 namenode, datanode, historyserver, resourcemanager 연결 
- 카프카 & 로그스태시 & 스파크 연결


**2022 / 05 / 16**

- Hadoop namenode, datanode, historyserver, resourcemanager 각각 구축 후 테스트
- 프로젝트에 맞게 도커파일, 도커이미지 수정


▶ 오늘의 오류 발생 및 해결 과정

	1. ssh 설치 후 비밀번호 입력 오류
		→  ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa 명령어 실행으로 해결
	
	2. namenode 실행 시 JAVA_HOME 설정 오류
		→ vi .bashrc 에서 export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64  추가

	3. error: resource mapping not found for name: "namenode" namespace: "default" from "namenode.yaml": no matches for kind "Deployment" in version "v1"
ensure CRDs are installed first	
		→ v1를 apps/v1로 변경 (서비스는 v1 그대로)
		
	4. error: error validating "namenode.yaml": error validating data: [ValidationError(Deployment.spec): unknown field "containers" in io.k8s.api.apps.v1.DeploymentSpec, ValidationError(Deployment.spec): unknown field "volumes" in io.k8s.api.apps.v1.DeploymentSpec]
		→ yaml파일 안에 spec: 빠져있었음, 넣은 다음 yaml문법에 맞게 들여쓰기 하기
		
	5. yaml 문법에 맞는 지 미리 확인이 필요	
		→ http://www.yamllint.com/ yaml 검사기 활용



**2022 / 05 / 17**

- hadoop 각 노드들끼리 ping, ssh 통신 연결
- ㅇㅇ


▶ 오늘의 오류 발생 및 해결 과정

	1. 22번 포트 연결 거절, 권한 없음 등 오류가 발생
		→  openssh server / client 재설치
		→ 
		
		
		


**2022 / 05 / 18**

- hadoop 각 노드들끼리 ping, ssh 통신 연결 진행중
- ㅇㅇ


▶ 오늘의 오류 발생 및 해결 과정

	1. 22번 포트 연결 거절, 권한 없음 등 오류가 계속 발생
		→  vi /etc/ssh/sshd_config
		→  apt-get install ufw
		→  vi /etc/hosts.allow
		→  passwd 설정
		→ 
		→ 



**2022 / 05 / 19**

- hadoop 각 노드들끼리 ping, ssh 통신 연결 (완료)
- ㅇㅇ


▶ 오늘의 오류 발생 및 해결 과정

	1. 포트 타임아웃 등 오류가 계속 발생
		→  ㅇㅇ
