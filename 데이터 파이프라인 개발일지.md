# 데이터 파이프라인 개발일지 ✨
▶ 완료  ⏳ 진행중  ⏱️ 예정  
-------------------------------------

### **2022 / 05 / 04**   
### ▶ 파이프라인 시나리오 작성  

  
### **2022 / 05 / 09**  
### ▶ 파이프라인 테스트 환경 구축 (테스트용)  
- openssh-server 설치  
- 포트포워딩 설정
- hadoop 설치
- hive 설치
- spark 설치
- kafka 설치
  
  
### **2022 / 05 / 10**  
### ▶ Spark streaming에서 증권사 주문 데이터 테스트
- 스트리밍 컨텍스트 인스턴스 생성  
- 이상 스트림 생성  
	✔ 예제 데이터 내려받기  
	✔ DStream 객체 생성  
- 이상 스트림 사용  
	✔ 데이터 파싱  
	✔ 거래 주문 건수 집계  
- 결과 파일로 저장  
- 스트리밍 계산 작업 시작 및 종료  
	✔ 스파크 스트리밍으로 데이터 전송  
	✔ 출력  
- 시간에 따라 변화하는 계산 상태 저장  
	✔ updateStateByKey로 상태 유지  
	✔ union으로 두 DStream 병합  
	✔ 체크포인팅 디렉터리 지정  
	✔ 두번째 출력  
	✔ mapWithState  
- 윈도 연산으로 일정 시간동안 유입된 데이터만 계산  
	✔ 마지막 지표 계산  
	
### **2022 / 05 / 12**  
### ⏳ 파이프라인 시나리오 수정 (진행중)
### 클라이언트들로부터 kafka로 메시지 수집 -> spark로 처리 -> ELK로 시각화  
- 플랫폼에 적용할 로그 데이터  
	✔ 사람들이 많이 모이는 상품 top 10 -> 상단노출  
	✔ 특정 시간에 제공하는 타임 쿠폰(모든 사용자)   
	-> 사람들이 주로 접속하는 시간대? 아님 반대로 줄어드는 시간대를 분석해서 결정  


- 예측을 위한 로그 데이터  
	✔ 사용자들이 자주 찾는 상품 (재입고처럼.. 자주 원하는 상품)  
	✔ 로그인 횟수(찜하기)와 결제 간의 상관관계   
	아니면 퍼널별 전환율 분석 (유입, 아이템조회, 찜하기, 구매) → 가장 가파르게 하락하는 구간의 전환율 계산  
	✔ 로그인을 하지 않은 유저가 처음으로 접속하는 페이지/상품(메인제외)  
	
### ⏳ 와이어 프레임 작성 (진행중)  
- 메인페이지  
	✔ 각 품목별 시세; 서울시농수산식품공사 공공데이터 - 주요 품목 가격 참고   
		도매시장 기준   
		날짜, 단위, 경락가, 반입량  
  
- 수급(유통)분석  
	✔ 가격동향; 서울시농수산식품공사 공공데이터 - 주요 품목 가격 참고  
	(품목별 카테고리 존재, 상세검색으로 날짜 설정하면 데이터도 그에 맞는걸로 출력)  
		주간수급예측가격, 2일전일가, 전일가, 차액정보(2일전일가-전일가), 그래프  
  
	✔ 수급상황; 농림축산식품 공공데이터 포털 - 전국 도매시장 일별 정산 경락가격 상세정보 참고 (오픈 API)  
	(품목별 카테고리 존재, 상세검색으로 날짜 설정 상관없음. 어차피 )  
		공급량(2022년, 평년누적), 1일전, 1개월전, 1년전, 그래프 (좌측 글씨를 누르면 우측 그래프에서 금년 공급량이 적용된 모습으로 변화)  

- 생산관측  
	✔ 생산통계; 농림축산식품 공공데이터 포털 - 전국 도매시장 일별 정산 경락가격 상세정보 참고 (오픈 API)  
	(품목별 카테고리 존재, 상세검색으로 날짜 설정하면 데이터도 그에 맞는걸로 출력)  
		타지역과 비교? 강원도, 충청도, 전라도, 경상도 별 유입물량 그래프  

	✔ 농가경영분석; 근데 이거 못할듯... ; 출처가 주요 농산물 생산실태 조사분석 모델링용역 결과 반영이라서  
		생산량, 판매단가, 총수입, 경영비, 생산비, 소득, 순이익(기준면적 300평)  
		소득률, 순수익률  

- 농업기상 기상정보 (하나로 결정할 것)  
	✔ 시군별 기상정보; 공공데이터 포털  
		지역별 기상현황으로 지역 클릭해서 기상정보 확인  
		현재 날씨, 기온, 풍속, 습도, 강수확률, 강수량  
		오늘, 내일, 모레 (날씨, 기온, 강수확률, 풍속, 강수량, 습도)  

	✔ 주산지 기상정보  
		품목(과일)이 재배되는 지역들의 날짜, 날씨(오전/오후), 온도(최고/평균/최저), 습도(최고/평균/최저), 풍속, 강수량(오전/오후), 강수확률(오전/오후)  

### ⏳ 로그 데이터 항목 설정 (진행중) 
- 사람들이 많이 모이는 상품 top 10  
	✔ 유저 id  
	✔ 과일(상품) 품목 코드  
	✔ 주문수량  
	✔ 주문 날짜 및 시간  
	✔ 주문 id  
	✔ 가격  
	✔ 구매유형 (단독, 공동)
	
### ▶ 농림축산식품 공공데이터 포털 (전국 도매시장 일별 정산 경락가격 상세정보) 오픈 api 신청 및 승인 완료  
### ▶ 서울시농수산식품공사 공공데이터 (주요 품목 가격) 신청 및 승인 완료  

### **2022 / 05 / 13**  
### ▶ 기상청_지상 종관 일자료, 시간자료 조회서비스 오픈 api 신청 및 출력결과 확인 (온도, 습도, 풍속, 강수량)  
### ▶ kafka - logstash - spark 데이터 전송 방법 찾기
- kafka - spark 방식으로 수정
### ▶ kafka - spark 커넥터 (리시버 기반 커넥터, 다이렉트 커넥터) 사용 고민하기


### **2022 / 05 / 17**  
### ▶ 서울시농수산식품공사 공공데이터 (유통정보(시장도매인거래)-품목별등급별가격) 신청 및 승인 완료  
### ▶ 서울시농수산식품공사 공공데이터 (유통정보-경매품목 물량분포) 신청 및 승인 완료  
### ▶ 서울시농수산식품공사 공공데이터 (유통정보-주간등락품목(주별가격흐름)) 신청 및 승인 완료  
### ⏳ 오픈 api에서 로그스태시로 데이터 불러오는 방법 찾기 
- 책 '엘라스틱 스택 개발부터 운영까지' 대여 및 참고 이후 결정
### ▶ 로그스태시 예제 (population.csv) 테스트

### **2022 / 05 / 18**  
### ⏱️ kafka - spark 커넥터 사용 예제 데스트 (예정)
- 카프카 시작  
- 카프카 사용해서 스트리밍 애플리케이션 개발  
	✔ 스파크-카프카 커넥터 사용  
	✔ 카프카로 메시지 전송  
	✔ 예제 실행  
### ⏳ 파이프라인 시나리오 수정 (진행중)
### ▶ 오픈 api에서 로그스태시로 데이터 불러오는 방법 찾기(진행중)  
- 문제  
	✔ 예제가 없어서 config 파일 작성이 어렵고, 에러 발생  
### ⏳ 로그스태시 fitter 코드 짜기 (기상정보, 가격 및 부가정보)  
- 기상청_지상(종관, ASOS) 시간자료 조회서비스 config 파일 내용  
```
input {
  http_poller {
    urls => {
      url => "http://apis.data.go.kr/1360000/AsosHourlyInfoService/getWthrDataList?serviceKey=인증키&dataType=JSON\
      &numOfRows=50&pageNo=1&dataCd=ASOS&dateCd=HR&stnIds=108&endDt=20220517&endHh=20&startHh=13&startDt=20220517"}
    request_timeout => 60
    schedule => { cron => "0 0 0/1 * * *"} # 1시간 마다 실행 
    codec => json { charset => "UTF-8" }
  }
}


filter{
  split {
    field => "item"
  }
  
  mutate {
    remove_field => ["[item][m03Te]", "[item][m02Te]", [item][m01Te]", "[item][m005Te]", 
    "[item][dmstMtphNo]","[item][gndSttCd]","[item][vs]", "[item][lcsCh]","[item][clfmAbbrCd]"
    , "[item][dc10LmcsCa]" , "[item][dc10Tca]","[item][hr3Fhsc]", "[item][dsnw]"
    ,  "[item][icsr]", "[item][ss]", "[item][ssQcflag]", "[item][ps]", "[item][psQcflag]"
    , "[item][pageRow]", "[item][curPage]", "[item][trainCo]", "[item][ordkey]"
    , "[item][pa]", "[item][paQcflag]", "[item][td]", "[item][pv]"], "[item][hmQcflag]", "[item][wdQcflag]"]
    , "[item][wsQcflag]", "[item][rnQcflag]", "[item][taQcflag]", "[item][tsQcflg]"]
    #remove_field => ["errorMessage","@timestamp","@version"]
  }
}

output {
  elasticsearch {
    hosts => "엘라스틱서치가 설치되어 있는 PC의 IP"
    index => "t_realtime_weather_list"
  }
}
```

- 문제  
	✔ runner - Logstash shut down. 으로 자꾸 로그스태시가 5초만에 꺼짐  
	
### **2022 / 05 / 19**  
### ▶ 스파크 스트리밍 애플리케이션 작성 코드 분석 (자세하게 한줄한줄)
### ▶ 몰랐던 메서드 및 용어 공부  
- textFileStream 메서드  
	✔스트리밍 처리 시작한 시점 이후에 폴더로 복사된 파일만 처리  
- splitAndSend.sh = 리눅스 셸스크립트  
	✔ 50만 건 데이터를 한 번에 시스템에 유입하는 것은 다소 비현실적.  
	✔ 셸 스크립트를 이용하여 스트리밍 데이터를 생성. HDFS로 복사하려면 권한이 있어야 함  
- DStream = 이산 스트림  
	✔ 스파크 스트리밍의 기본 추상화 객체, 입력 데이터 스트림에서 주기적으로 생성하는 일련의 RDD 시퀀스   
- union 메서드  
	✔ DStream 병합 가능. 근데 요소 타입이 서로 동일해야 함  
- shell 스크립트 = .sh 파일  
	✔ #!/bin/bash로 시작하는 파일  
	
### ▶ 로그 데이터 - 카프카 - 스파크 스트리밍 - 카프카 파이프라인 구성
1. shell 스크립트로 데이터를 카프카 토픽으로 전송  
2. 스파크 스트리밍은 토픽에서 데이터 읽기  
3. 각 지표의 계산결과는 다시 또 다른 카프카 토픽으로 전송  
4. 카프카는 컨슈머 스크립트를 사용해서 지표 결과 수신 및 출력  

### **2022 / 05 / 20** 
### ⏳ 로그 데이터 conf 코드 수정  

### **2022 / 05 / 25** 
### ⏳ 카프카 - 스파크 (INFO shutting down (kafka.server.KafkaServer))  
- kafka broker기동시 brokerId가 달라서 기동에 실패  
	✔ config/server.properties 파일에 broker.id를 수정 후 재시작했다가 다시 broker.id를 수정  
	✔ logs파일 위치(예, "/tmp/kafka-logs/meta.properties")에 있는 meta.properties파일을 열어서 broker.id를 수정하고 저장  
	
	``` 
	#A comma seperated list of directories under which to store log files
	log.dirs=/tmp/kafka-logs   
	
	
	 $ cat /tmp/kafka-logs/meta.properties
	#Tue May 24 17:53:38 KST 2022
	version=0
	broker.id=0   
	
	
	실행하고 다시 kafka를 실행하면 더 이상 위의 에러는 발생하지 않음
	$ rm /tmp/kafka-logs/meta.properties 
	```  
	
### **2022 / 05 / 26** 
### ⏳ 카프카 - 스파크
- 카프카에서 데이터 불러서 스파크에 출력하는것까지 완료
- 스파크 내에서 계산한 결과 하둡(로컬)에 저장하기
