# 데이터 파이프라인 개발일지 ✨
▶ 완료  ⏳ 진행중  ⏱️ 예정  
-------------------------------------

### **2022 / 05 / 04**   
### ▶ 파이프라인 시나리오 작성  

  
### **2022 / 05 / 09**  
### ▶ 파이프라인 테스트 환경 구축 (테스트용)  
- openssh-server 설치  
- 포트포워딩 설정
- hadoop 설치
- hive 설치
- spark 설치
- kafka 설치
  
  
### **2022 / 05 / 10**  
### ▶ Spark streaming에서 증권사 주문 데이터 테스트
- 스트리밍 컨텍스트 인스턴스 생성  
- 이상 스트림 생성  
	✔ 예제 데이터 내려받기  
	✔ DStream 객체 생성  
- 이상 스트림 사용  
	✔ 데이터 파싱  
	✔ 거래 주문 건수 집계  
- 결과 파일로 저장  
- 스트리밍 계산 작업 시작 및 종료  
	✔ 스파크 스트리밍으로 데이터 전송  
	✔ 출력  
- 시간에 따라 변화하는 계산 상태 저장  
	✔ updateStateByKey로 상태 유지  
	✔ union으로 두 DStream 병합  
	✔ 체크포인팅 디렉터리 지정  
	✔ 두번째 출력  
	✔ mapWithState  
- 윈도 연산으로 일정 시간동안 유입된 데이터만 계산  
	✔ 마지막 지표 계산  
	
### **2022 / 05 / 12**  
### ⏳ 파이프라인 시나리오 수정 (진행중)
### 클라이언트들로부터 kafka로 메시지 수집 -> spark로 처리 -> ELK로 시각화  
- 플랫폼에 적용할 로그 데이터  
	✔ 사람들이 많이 모이는 상품 top 10 -> 상단노출  
	✔ 특정 시간에 제공하는 타임 쿠폰(모든 사용자)   
	-> 사람들이 주로 접속하는 시간대? 아님 반대로 줄어드는 시간대를 분석해서 결정  


- 예측을 위한 로그 데이터  
	✔ 사용자들이 자주 찾는 상품 (재입고처럼.. 자주 원하는 상품)  
	✔ 로그인 횟수(찜하기)와 결제 간의 상관관계   
	아니면 퍼널별 전환율 분석 (유입, 아이템조회, 찜하기, 구매) → 가장 가파르게 하락하는 구간의 전환율 계산  
	✔ 로그인을 하지 않은 유저가 처음으로 접속하는 페이지/상품(메인제외)  
	
### ⏳ 와이어 프레임 작성 (진행중)  
- 메인페이지  
	✔ 각 품목별 시세; 서울시농수산식품공사 공공데이터 - 주요 품목 가격 참고   
		도매시장 기준   
		날짜, 단위, 경락가, 반입량  
  
- 수급(유통)분석  
	✔ 가격동향; 서울시농수산식품공사 공공데이터 - 주요 품목 가격 참고  
	(품목별 카테고리 존재, 상세검색으로 날짜 설정하면 데이터도 그에 맞는걸로 출력)  
		주간수급예측가격, 2일전일가, 전일가, 차액정보(2일전일가-전일가), 그래프  
  
	✔ 수급상황; 농림축산식품 공공데이터 포털 - 전국 도매시장 일별 정산 경락가격 상세정보 참고 (오픈 API)  
	(품목별 카테고리 존재, 상세검색으로 날짜 설정 상관없음. 어차피 )  
		공급량(2022년, 평년누적), 1일전, 1개월전, 1년전, 그래프 (좌측 글씨를 누르면 우측 그래프에서 금년 공급량이 적용된 모습으로 변화)  

- 생산관측  
	✔ 생산통계; 농림축산식품 공공데이터 포털 - 전국 도매시장 일별 정산 경락가격 상세정보 참고 (오픈 API)  
	(품목별 카테고리 존재, 상세검색으로 날짜 설정하면 데이터도 그에 맞는걸로 출력)  
		타지역과 비교? 강원도, 충청도, 전라도, 경상도 별 유입물량 그래프  

	✔ 농가경영분석; 근데 이거 못할듯... ; 출처가 주요 농산물 생산실태 조사분석 모델링용역 결과 반영이라서  
		생산량, 판매단가, 총수입, 경영비, 생산비, 소득, 순이익(기준면적 300평)  
		소득률, 순수익률  

- 농업기상 기상정보 (하나로 결정할 것)  
	✔ 시군별 기상정보; 공공데이터 포털  
		지역별 기상현황으로 지역 클릭해서 기상정보 확인  
		현재 날씨, 기온, 풍속, 습도, 강수확률, 강수량  
		오늘, 내일, 모레 (날씨, 기온, 강수확률, 풍속, 강수량, 습도)  

	✔ 주산지 기상정보  
		품목(과일)이 재배되는 지역들의 날짜, 날씨(오전/오후), 온도(최고/평균/최저), 습도(최고/평균/최저), 풍속, 강수량(오전/오후), 강수확률(오전/오후)  

### ⏳ 로그 데이터 항목 설정 (진행중) 
- 사람들이 많이 모이는 상품 top 10  
	✔ 유저 id  
	✔ 과일(상품) 품목 코드  
	✔ 주문수량  
	✔ 주문 날짜 및 시간  
	✔ 주문 id  
	✔ 가격  
	✔ 구매유형 (단독, 공동)
	
### ▶ 농림축산식품 공공데이터 포털 (전국 도매시장 일별 정산 경락가격 상세정보) 오픈 api 신청 및 승인 완료  
### ▶ 서울시농수산식품공사 공공데이터 (주요 품목 가격) 신청 및 승인 완료  

### **2022 / 05 / 13**  
### ▶ 기상청_지상 종관 일자료, 시간자료 조회서비스 오픈 api 신청 및 출력결과 확인 (온도, 습도, 풍속, 강수량)  
### ▶ kafka - logstash - spark 데이터 전송 방법 찾기
- kafka - spark 방식으로 수정
### ▶ kafka - spark 커넥터 (리시버 기반 커넥터, 다이렉트 커넥터) 사용 고민하기


### **2022 / 05 / 17**  
### ▶ 서울시농수산식품공사 공공데이터 (유통정보(시장도매인거래)-품목별등급별가격) 신청 및 승인 완료  
### ▶ 서울시농수산식품공사 공공데이터 (유통정보-경매품목 물량분포) 신청 및 승인 완료  
### ▶ 서울시농수산식품공사 공공데이터 (유통정보-주간등락품목(주별가격흐름)) 신청 및 승인 완료  
### ⏳ 오픈 api에서 로그스태시로 데이터 불러오는 방법 찾기 
- 책 '엘라스틱 스택 개발부터 운영까지' 대여 및 참고 이후 결정
### ▶ 로그스태시 예제 (population.csv) 테스트

### **2022 / 05 / 18**  
### ⏱️ kafka - spark 커넥터 사용 예제 데스트 (예정)
- 카프카 시작  
- 카프카 사용해서 스트리밍 애플리케이션 개발  
	✔ 스파크-카프카 커넥터 사용  
	✔ 카프카로 메시지 전송  
	✔ 예제 실행  
### ⏳ 파이프라인 시나리오 수정 (진행중)
### ▶ 오픈 api에서 로그스태시로 데이터 불러오는 방법 찾기(진행중)  
- 문제  
	✔ 예제가 없어서 config 파일 작성이 어렵고, 에러 발생  
### ⏳ 로그스태시 fitter 코드 짜기 (기상정보, 가격 및 부가정보)  
- 기상청_지상(종관, ASOS) 시간자료 조회서비스 config 파일 내용  
```
input {
  http_poller {
    urls => {
      url => "http://apis.data.go.kr/1360000/AsosHourlyInfoService/getWthrDataList?serviceKey=인증키&dataType=JSON\
      &numOfRows=50&pageNo=1&dataCd=ASOS&dateCd=HR&stnIds=108&endDt=20220517&endHh=20&startHh=13&startDt=20220517"}
    request_timeout => 60
    schedule => { cron => "0 0 0/1 * * *"} # 1시간 마다 실행 
    codec => json { charset => "UTF-8" }
  }
}


filter{
  split {
    field => "item"
  }
  
  mutate {
    remove_field => ["[item][m03Te]", "[item][m02Te]", [item][m01Te]", "[item][m005Te]", 
    "[item][dmstMtphNo]","[item][gndSttCd]","[item][vs]", "[item][lcsCh]","[item][clfmAbbrCd]"
    , "[item][dc10LmcsCa]" , "[item][dc10Tca]","[item][hr3Fhsc]", "[item][dsnw]"
    ,  "[item][icsr]", "[item][ss]", "[item][ssQcflag]", "[item][ps]", "[item][psQcflag]"
    , "[item][pageRow]", "[item][curPage]", "[item][trainCo]", "[item][ordkey]"
    , "[item][pa]", "[item][paQcflag]", "[item][td]", "[item][pv]"], "[item][hmQcflag]", "[item][wdQcflag]"]
    , "[item][wsQcflag]", "[item][rnQcflag]", "[item][taQcflag]", "[item][tsQcflg]"]
    #remove_field => ["errorMessage","@timestamp","@version"]
  }
}

output {
  elasticsearch {
    hosts => "엘라스틱서치가 설치되어 있는 PC의 IP"
    index => "t_realtime_weather_list"
  }
}
```

- 문제  
	✔ runner - Logstash shut down. 으로 자꾸 로그스태시가 5초만에 꺼짐  
	
### **2022 / 05 / 19**  
### ▶ 스파크 스트리밍 애플리케이션 작성 코드 분석 (자세하게 한줄한줄)
### ▶ 몰랐던 메서드 및 용어 공부  
- textFileStream 메서드  
	✔스트리밍 처리 시작한 시점 이후에 폴더로 복사된 파일만 처리  
- splitAndSend.sh = 리눅스 셸스크립트  
	✔ 50만 건 데이터를 한 번에 시스템에 유입하는 것은 다소 비현실적.  
	✔ 셸 스크립트를 이용하여 스트리밍 데이터를 생성. HDFS로 복사하려면 권한이 있어야 함  
- DStream = 이산 스트림  
	✔ 스파크 스트리밍의 기본 추상화 객체, 입력 데이터 스트림에서 주기적으로 생성하는 일련의 RDD 시퀀스   
- union 메서드  
	✔ DStream 병합 가능. 근데 요소 타입이 서로 동일해야 함  
- shell 스크립트 = .sh 파일  
	✔ #!/bin/bash로 시작하는 파일  
	
### ▶ 로그 데이터 - 카프카 - 스파크 스트리밍 - 카프카 파이프라인 구성
1. shell 스크립트로 데이터를 카프카 토픽으로 전송  
2. 스파크 스트리밍은 토픽에서 데이터 읽기  
3. 각 지표의 계산결과는 다시 또 다른 카프카 토픽으로 전송  
4. 카프카는 컨슈머 스크립트를 사용해서 지표 결과 수신 및 출력  

### **2022 / 05 / 20** 
### ⏳ 로그 데이터 conf 코드 수정  

### **2022 / 05 / 25** 
### ⏳ 카프카 - 스파크 (INFO shutting down (kafka.server.KafkaServer))  
- 문제 (kafka broker기동시 brokerId가 달라서 기동에 실패)  
	✔ config/server.properties 파일에 broker.id를 수정 후 재시작했다가 다시 broker.id를 수정  
	✔ logs파일 위치(예, "/tmp/kafka-logs/meta.properties")에 있는 meta.properties파일을 열어서 broker.id를 수정하고 저장  
	
	``` 
	#A comma seperated list of directories under which to store log files
	log.dirs=/tmp/kafka-logs   
	
	
	 $ cat /tmp/kafka-logs/meta.properties
	#Tue May 24 17:53:38 KST 2022
	version=0
	broker.id=0   
	
	
	실행하고 다시 kafka를 실행하면 더 이상 위의 에러는 발생하지 않음
	$ rm /tmp/kafka-logs/meta.properties 
	```  
- 스파크 코드 수정 (scala → pyspark)  
	❌ pyspark --master local[4] --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.1  
	✔ pyspark --master local[4] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1  
	★★★★ streaming은 옛날버전에서만 제공하기 때문에 이제는 sql 사용할 것 ★★★★   
	
```
pyspark --master local[4] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1

import os
os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1'

kafka_df = spark.readStream.   format("kafka").\
	option("subscribe", "test").\
	option("kafka.bootstrap.servers", "192.168.153.155:9092").\
	load()

query = kafka_df.writeStream.format("console").start()

kafka_df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")


# 이게 문제
ds = kafka_df \
   .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
   .writeStream \
   .format("kafka") \
   .option("kafka.bootstrap.servers", "192.168.153.155:9092") \
   .option("topic", "test") \
   .start()

# .option("checkpointLocation", "/tmp") \ 한줄 추가. 하지만 실패
ds = kafka_df \
   .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
   .writeStream \
   .format("kafka") \
   .option("kafka.bootstrap.servers", "192.168.153.155:9092") \
   .option("checkpointLocation", "/tmp") \
   .option("topic", "test") \
   .start()

# 이것도 실패
ds = kafka_df \
   .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
   .writeStream \
   .format("console") \
   .start()

# 이것도 실패
ds = kafka_df \
   .writeStream \
   .format("console") \
   .start()

# 이것도 실패인듯??
ds = kafka_df \
   .writeStream \
   .format("console") \
   .option("checkpointLocation", "/tmp") \
   .start()

# 이것도 실패
df.show()
df = spark.readStream.\
   format("kafka").\
   option("subscribe", "test").\
   option("kafka.bootstrap.servers", "192.168.153.155:9092").\
   option("startingOffsets", "earliest") \
   load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
df.printSchema()

# 이것도 실패
df = spark.readStream.\
   format("kafka").\
   option("subscribe", "test").\
   option("kafka.bootstrap.servers", "192.168.153.155:9092").\
   option("startingOffsets", "earliest") .\
   load()
 df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
DataFrame[key: string, value: string]
df.printSchema()



# 포맷 콘솔로 변경. 이거 뭔가 반응있음
query = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
     .writeStream \
     .format("console") \
     .start()

# 채크포인트 경로 설정. 이것도 실패
query = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
     .writeStream \
     .format("console") \
     .option("checkpointLocation", "/tmp") \
     .start()

# 이게 진짜??
query = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
     .writeStream \
     .format("console") \
     .option("checkpointLocation", "/tmp") \
     .start()
```
	
### **2022 / 05 / 26** 
### ⏳ 카프카 - 스파크
- 카프카에서 데이터 불러서 스파크에 출력하는것까지 완료
- 스파크 내에서 계산한 결과 하둡(로컬)에 저장하기
- 스파크 
	✔ pyspark --master local[4]

```
>>> import os

🔔 kafka-clients-3.1.0.jar와 commons-pool2-2.11.1.jar와 spark-token-provider-kafka-0-10_2.12-3.2.1.jar 다운받고 추가
>>> os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,kafka-clients-3.1.0.jar,commons-pool2-2.11.1.jar,spark-token-provider-kafka-0-10_2.12-3.2.1.jar'
>>> 
>>> 
>>> df = spark.readStream.\
...   format("kafka").\
...   option("subscribe", "test").\
...   option("kafka.bootstrap.servers", "192.168.153.155:9092").\
...   option("startingOffsets", "earliest") .\
...   load()

>>> df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
DataFrame[key: string, value: string]
>>> df.printSchema()
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

>>> 
>>> query = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
...     .writeStream \
...     .format("console") \
...     .option("checkpointLocation", "/tmp") \
...     .start()
22/05/26 10:28:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
>>> 
>>> 
>>> type(query)
<class 'pyspark.sql.streaming.StreamingQuery'>
>>> df.show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/spark/python/pyspark/sql/dataframe.py", line 494, in show
    print(self._jdf.showString(n, 20, vertical))
  File "/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: Queries with streaming sources must be executed with writeStream.start();
kafka
>>> 
>>> 
>>> df.show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/spark/python/pyspark/sql/dataframe.py", line 494, in show
    print(self._jdf.showString(n, 20, vertical))
  File "/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: Queries with streaming sources must be executed with writeStream.start();
kafka
>>> df.select(df.value)
DataFrame[value: binary]
>>> df.select(df.topic)
DataFrame[topic: string]
>>> query.awaitTermination()
-------------------------------------------                                     
Batch: 0
-------------------------------------------
+---+-----+
|key|value|
+---+-----+
+---+-----+

-------------------------------------------                                     
Batch: 1
-------------------------------------------
+----+-----+
| key|value|
+----+-----+
|null|     |
|null|     |
|null|     |
+----+-----+

-------------------------------------------                                     
Batch: 2
-------------------------------------------
+----+-----+
| key|value|
+----+-----+
|null| test|
|null| qwer|
+----+-----+


```

### **2022 / 05 / 31** 
### ⏳ 카프카 - 스파크 문제상황
1. 카프카 서버 실행 안됨 (계속 셧다운되고, ip가 사라지는데 이유 모르겠음)
2. 6월 1일 테스트 겸 실행할 때 어떤 컴으로 원격접속? 로그 데이터가 카프카로 들어와도 현재 연결된 컴퓨터는 실행 불가
3. 스파크에서 카프카 호출하는 코드랑 데이터 연산하는 코드를 하나로 합쳐봤는데 1번 때문에 테스트 못 해봄

### **2022 / 06 / 03** 
### ⏳ 카프카에서 받은 데이터 스파크에서 실행할 때 문제사항
- df.show 하거나 .save로 실행하면 엄청 긴 에러 메세지 출력 → 꼼꼼히 살펴봐서 java.net.UnknownHostException 에러 확인하고 해결  
	✔ vi /etc/hosts 에 카프카 브로커 ip랑 카프카 hostname 추가
	```
	✔ 192.168.182.19 kafka-646c48c49b-69tvs
	❌ 127.0.0.1 kafka-646c48c49b-69tvs
	❌ 127.0.0.1 hadoop-spark-zeppelin-665894bc4d-4hdxq
	```
