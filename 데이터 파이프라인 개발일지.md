# ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê°œë°œì¼ì§€ âœ¨
â–¶ ì™„ë£Œ  â³ ì§„í–‰ì¤‘  â±ï¸ ì˜ˆì •  
-------------------------------------

### **2022 / 05 / 04**   
### â–¶ íŒŒì´í”„ë¼ì¸ ì‹œë‚˜ë¦¬ì˜¤ ì‘ì„±  

  
### **2022 / 05 / 09**  
### â–¶ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶• (í…ŒìŠ¤íŠ¸ìš©)  
- openssh-server ì„¤ì¹˜  
- í¬íŠ¸í¬ì›Œë”© ì„¤ì •
- hadoop ì„¤ì¹˜
- hive ì„¤ì¹˜
- spark ì„¤ì¹˜
- kafka ì„¤ì¹˜
  
  
### **2022 / 05 / 10**  
### â–¶ Spark streamingì—ì„œ ì¦ê¶Œì‚¬ ì£¼ë¬¸ ë°ì´í„° í…ŒìŠ¤íŠ¸
- ìŠ¤íŠ¸ë¦¬ë° ì»¨í…ìŠ¤íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±  
- ì´ìƒ ìŠ¤íŠ¸ë¦¼ ìƒì„±  
	âœ” ì˜ˆì œ ë°ì´í„° ë‚´ë ¤ë°›ê¸°  
	âœ” DStream ê°ì²´ ìƒì„±  
- ì´ìƒ ìŠ¤íŠ¸ë¦¼ ì‚¬ìš©  
	âœ” ë°ì´í„° íŒŒì‹±  
	âœ” ê±°ë˜ ì£¼ë¬¸ ê±´ìˆ˜ ì§‘ê³„  
- ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥  
- ìŠ¤íŠ¸ë¦¬ë° ê³„ì‚° ì‘ì—… ì‹œì‘ ë° ì¢…ë£Œ  
	âœ” ìŠ¤íŒŒí¬ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°ì´í„° ì „ì†¡  
	âœ” ì¶œë ¥  
- ì‹œê°„ì— ë”°ë¼ ë³€í™”í•˜ëŠ” ê³„ì‚° ìƒíƒœ ì €ì¥  
	âœ” updateStateByKeyë¡œ ìƒíƒœ ìœ ì§€  
	âœ” unionìœ¼ë¡œ ë‘ DStream ë³‘í•©  
	âœ” ì²´í¬í¬ì¸íŒ… ë””ë ‰í„°ë¦¬ ì§€ì •  
	âœ” ë‘ë²ˆì§¸ ì¶œë ¥  
	âœ” mapWithState  
- ìœˆë„ ì—°ì‚°ìœ¼ë¡œ ì¼ì • ì‹œê°„ë™ì•ˆ ìœ ì…ëœ ë°ì´í„°ë§Œ ê³„ì‚°  
	âœ” ë§ˆì§€ë§‰ ì§€í‘œ ê³„ì‚°  
	
### **2022 / 05 / 12**  
### â³ íŒŒì´í”„ë¼ì¸ ì‹œë‚˜ë¦¬ì˜¤ ìˆ˜ì • (ì§„í–‰ì¤‘)
### í´ë¼ì´ì–¸íŠ¸ë“¤ë¡œë¶€í„° kafkaë¡œ ë©”ì‹œì§€ ìˆ˜ì§‘ -> sparkë¡œ ì²˜ë¦¬ -> ELKë¡œ ì‹œê°í™”  
- í”Œë«í¼ì— ì ìš©í•  ë¡œê·¸ ë°ì´í„°  
	âœ” ì‚¬ëŒë“¤ì´ ë§ì´ ëª¨ì´ëŠ” ìƒí’ˆ top 10 -> ìƒë‹¨ë…¸ì¶œ  
	âœ” íŠ¹ì • ì‹œê°„ì— ì œê³µí•˜ëŠ” íƒ€ì„ ì¿ í°(ëª¨ë“  ì‚¬ìš©ì)   
	-> ì‚¬ëŒë“¤ì´ ì£¼ë¡œ ì ‘ì†í•˜ëŠ” ì‹œê°„ëŒ€? ì•„ë‹˜ ë°˜ëŒ€ë¡œ ì¤„ì–´ë“œëŠ” ì‹œê°„ëŒ€ë¥¼ ë¶„ì„í•´ì„œ ê²°ì •  


- ì˜ˆì¸¡ì„ ìœ„í•œ ë¡œê·¸ ë°ì´í„°  
	âœ” ì‚¬ìš©ìë“¤ì´ ìì£¼ ì°¾ëŠ” ìƒí’ˆ (ì¬ì…ê³ ì²˜ëŸ¼.. ìì£¼ ì›í•˜ëŠ” ìƒí’ˆ)  
	âœ” ë¡œê·¸ì¸ íšŸìˆ˜(ì°œí•˜ê¸°)ì™€ ê²°ì œ ê°„ì˜ ìƒê´€ê´€ê³„   
	ì•„ë‹ˆë©´ í¼ë„ë³„ ì „í™˜ìœ¨ ë¶„ì„ (ìœ ì…, ì•„ì´í…œì¡°íšŒ, ì°œí•˜ê¸°, êµ¬ë§¤) â†’ ê°€ì¥ ê°€íŒŒë¥´ê²Œ í•˜ë½í•˜ëŠ” êµ¬ê°„ì˜ ì „í™˜ìœ¨ ê³„ì‚°  
	âœ” ë¡œê·¸ì¸ì„ í•˜ì§€ ì•Šì€ ìœ ì €ê°€ ì²˜ìŒìœ¼ë¡œ ì ‘ì†í•˜ëŠ” í˜ì´ì§€/ìƒí’ˆ(ë©”ì¸ì œì™¸)  
	
### â³ ì™€ì´ì–´ í”„ë ˆì„ ì‘ì„± (ì§„í–‰ì¤‘)  
- ë©”ì¸í˜ì´ì§€  
	âœ” ê° í’ˆëª©ë³„ ì‹œì„¸; ì„œìš¸ì‹œë†ìˆ˜ì‚°ì‹í’ˆê³µì‚¬ ê³µê³µë°ì´í„° - ì£¼ìš” í’ˆëª© ê°€ê²© ì°¸ê³    
		ë„ë§¤ì‹œì¥ ê¸°ì¤€   
		ë‚ ì§œ, ë‹¨ìœ„, ê²½ë½ê°€, ë°˜ì…ëŸ‰  
  
- ìˆ˜ê¸‰(ìœ í†µ)ë¶„ì„  
	âœ” ê°€ê²©ë™í–¥; ì„œìš¸ì‹œë†ìˆ˜ì‚°ì‹í’ˆê³µì‚¬ ê³µê³µë°ì´í„° - ì£¼ìš” í’ˆëª© ê°€ê²© ì°¸ê³   
	(í’ˆëª©ë³„ ì¹´í…Œê³ ë¦¬ ì¡´ì¬, ìƒì„¸ê²€ìƒ‰ìœ¼ë¡œ ë‚ ì§œ ì„¤ì •í•˜ë©´ ë°ì´í„°ë„ ê·¸ì— ë§ëŠ”ê±¸ë¡œ ì¶œë ¥)  
		ì£¼ê°„ìˆ˜ê¸‰ì˜ˆì¸¡ê°€ê²©, 2ì¼ì „ì¼ê°€, ì „ì¼ê°€, ì°¨ì•¡ì •ë³´(2ì¼ì „ì¼ê°€-ì „ì¼ê°€), ê·¸ë˜í”„  
  
	âœ” ìˆ˜ê¸‰ìƒí™©; ë†ë¦¼ì¶•ì‚°ì‹í’ˆ ê³µê³µë°ì´í„° í¬í„¸ - ì „êµ­ ë„ë§¤ì‹œì¥ ì¼ë³„ ì •ì‚° ê²½ë½ê°€ê²© ìƒì„¸ì •ë³´ ì°¸ê³  (ì˜¤í”ˆ API)  
	(í’ˆëª©ë³„ ì¹´í…Œê³ ë¦¬ ì¡´ì¬, ìƒì„¸ê²€ìƒ‰ìœ¼ë¡œ ë‚ ì§œ ì„¤ì • ìƒê´€ì—†ìŒ. ì–´ì°¨í”¼ )  
		ê³µê¸‰ëŸ‰(2022ë…„, í‰ë…„ëˆ„ì ), 1ì¼ì „, 1ê°œì›”ì „, 1ë…„ì „, ê·¸ë˜í”„ (ì¢Œì¸¡ ê¸€ì”¨ë¥¼ ëˆ„ë¥´ë©´ ìš°ì¸¡ ê·¸ë˜í”„ì—ì„œ ê¸ˆë…„ ê³µê¸‰ëŸ‰ì´ ì ìš©ëœ ëª¨ìŠµìœ¼ë¡œ ë³€í™”)  

- ìƒì‚°ê´€ì¸¡  
	âœ” ìƒì‚°í†µê³„; ë†ë¦¼ì¶•ì‚°ì‹í’ˆ ê³µê³µë°ì´í„° í¬í„¸ - ì „êµ­ ë„ë§¤ì‹œì¥ ì¼ë³„ ì •ì‚° ê²½ë½ê°€ê²© ìƒì„¸ì •ë³´ ì°¸ê³  (ì˜¤í”ˆ API)  
	(í’ˆëª©ë³„ ì¹´í…Œê³ ë¦¬ ì¡´ì¬, ìƒì„¸ê²€ìƒ‰ìœ¼ë¡œ ë‚ ì§œ ì„¤ì •í•˜ë©´ ë°ì´í„°ë„ ê·¸ì— ë§ëŠ”ê±¸ë¡œ ì¶œë ¥)  
		íƒ€ì§€ì—­ê³¼ ë¹„êµ? ê°•ì›ë„, ì¶©ì²­ë„, ì „ë¼ë„, ê²½ìƒë„ ë³„ ìœ ì…ë¬¼ëŸ‰ ê·¸ë˜í”„  

	âœ” ë†ê°€ê²½ì˜ë¶„ì„; ê·¼ë° ì´ê±° ëª»í• ë“¯... ; ì¶œì²˜ê°€ ì£¼ìš” ë†ì‚°ë¬¼ ìƒì‚°ì‹¤íƒœ ì¡°ì‚¬ë¶„ì„ ëª¨ë¸ë§ìš©ì—­ ê²°ê³¼ ë°˜ì˜ì´ë¼ì„œ  
		ìƒì‚°ëŸ‰, íŒë§¤ë‹¨ê°€, ì´ìˆ˜ì…, ê²½ì˜ë¹„, ìƒì‚°ë¹„, ì†Œë“, ìˆœì´ìµ(ê¸°ì¤€ë©´ì  300í‰)  
		ì†Œë“ë¥ , ìˆœìˆ˜ìµë¥   

- ë†ì—…ê¸°ìƒ ê¸°ìƒì •ë³´ (í•˜ë‚˜ë¡œ ê²°ì •í•  ê²ƒ)  
	âœ” ì‹œêµ°ë³„ ê¸°ìƒì •ë³´; ê³µê³µë°ì´í„° í¬í„¸  
		ì§€ì—­ë³„ ê¸°ìƒí˜„í™©ìœ¼ë¡œ ì§€ì—­ í´ë¦­í•´ì„œ ê¸°ìƒì •ë³´ í™•ì¸  
		í˜„ì¬ ë‚ ì”¨, ê¸°ì˜¨, í’ì†, ìŠµë„, ê°•ìˆ˜í™•ë¥ , ê°•ìˆ˜ëŸ‰  
		ì˜¤ëŠ˜, ë‚´ì¼, ëª¨ë ˆ (ë‚ ì”¨, ê¸°ì˜¨, ê°•ìˆ˜í™•ë¥ , í’ì†, ê°•ìˆ˜ëŸ‰, ìŠµë„)  

	âœ” ì£¼ì‚°ì§€ ê¸°ìƒì •ë³´  
		í’ˆëª©(ê³¼ì¼)ì´ ì¬ë°°ë˜ëŠ” ì§€ì—­ë“¤ì˜ ë‚ ì§œ, ë‚ ì”¨(ì˜¤ì „/ì˜¤í›„), ì˜¨ë„(ìµœê³ /í‰ê· /ìµœì €), ìŠµë„(ìµœê³ /í‰ê· /ìµœì €), í’ì†, ê°•ìˆ˜ëŸ‰(ì˜¤ì „/ì˜¤í›„), ê°•ìˆ˜í™•ë¥ (ì˜¤ì „/ì˜¤í›„)  

### â³ ë¡œê·¸ ë°ì´í„° í•­ëª© ì„¤ì • (ì§„í–‰ì¤‘) 
- ì‚¬ëŒë“¤ì´ ë§ì´ ëª¨ì´ëŠ” ìƒí’ˆ top 10  
	âœ” ìœ ì € id  
	âœ” ê³¼ì¼(ìƒí’ˆ) í’ˆëª© ì½”ë“œ  
	âœ” ì£¼ë¬¸ìˆ˜ëŸ‰  
	âœ” ì£¼ë¬¸ ë‚ ì§œ ë° ì‹œê°„  
	âœ” ì£¼ë¬¸ id  
	âœ” ê°€ê²©  
	âœ” êµ¬ë§¤ìœ í˜• (ë‹¨ë…, ê³µë™)
	
### â–¶ ë†ë¦¼ì¶•ì‚°ì‹í’ˆ ê³µê³µë°ì´í„° í¬í„¸ (ì „êµ­ ë„ë§¤ì‹œì¥ ì¼ë³„ ì •ì‚° ê²½ë½ê°€ê²© ìƒì„¸ì •ë³´) ì˜¤í”ˆ api ì‹ ì²­ ë° ìŠ¹ì¸ ì™„ë£Œ  
### â–¶ ì„œìš¸ì‹œë†ìˆ˜ì‚°ì‹í’ˆê³µì‚¬ ê³µê³µë°ì´í„° (ì£¼ìš” í’ˆëª© ê°€ê²©) ì‹ ì²­ ë° ìŠ¹ì¸ ì™„ë£Œ  

### **2022 / 05 / 13**  
### â–¶ ê¸°ìƒì²­_ì§€ìƒ ì¢…ê´€ ì¼ìë£Œ, ì‹œê°„ìë£Œ ì¡°íšŒì„œë¹„ìŠ¤ ì˜¤í”ˆ api ì‹ ì²­ ë° ì¶œë ¥ê²°ê³¼ í™•ì¸ (ì˜¨ë„, ìŠµë„, í’ì†, ê°•ìˆ˜ëŸ‰)  
### â–¶ kafka - logstash - spark ë°ì´í„° ì „ì†¡ ë°©ë²• ì°¾ê¸°
- kafka - spark ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
### â–¶ kafka - spark ì»¤ë„¥í„° (ë¦¬ì‹œë²„ ê¸°ë°˜ ì»¤ë„¥í„°, ë‹¤ì´ë ‰íŠ¸ ì»¤ë„¥í„°) ì‚¬ìš© ê³ ë¯¼í•˜ê¸°


### **2022 / 05 / 17**  
### â–¶ ì„œìš¸ì‹œë†ìˆ˜ì‚°ì‹í’ˆê³µì‚¬ ê³µê³µë°ì´í„° (ìœ í†µì •ë³´(ì‹œì¥ë„ë§¤ì¸ê±°ë˜)-í’ˆëª©ë³„ë“±ê¸‰ë³„ê°€ê²©) ì‹ ì²­ ë° ìŠ¹ì¸ ì™„ë£Œ  
### â–¶ ì„œìš¸ì‹œë†ìˆ˜ì‚°ì‹í’ˆê³µì‚¬ ê³µê³µë°ì´í„° (ìœ í†µì •ë³´-ê²½ë§¤í’ˆëª© ë¬¼ëŸ‰ë¶„í¬) ì‹ ì²­ ë° ìŠ¹ì¸ ì™„ë£Œ  
### â–¶ ì„œìš¸ì‹œë†ìˆ˜ì‚°ì‹í’ˆê³µì‚¬ ê³µê³µë°ì´í„° (ìœ í†µì •ë³´-ì£¼ê°„ë“±ë½í’ˆëª©(ì£¼ë³„ê°€ê²©íë¦„)) ì‹ ì²­ ë° ìŠ¹ì¸ ì™„ë£Œ  
### â³ ì˜¤í”ˆ apiì—ì„œ ë¡œê·¸ìŠ¤íƒœì‹œë¡œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²• ì°¾ê¸° 
- ì±… 'ì—˜ë¼ìŠ¤í‹± ìŠ¤íƒ ê°œë°œë¶€í„° ìš´ì˜ê¹Œì§€' ëŒ€ì—¬ ë° ì°¸ê³  ì´í›„ ê²°ì •
### â–¶ ë¡œê·¸ìŠ¤íƒœì‹œ ì˜ˆì œ (population.csv) í…ŒìŠ¤íŠ¸

### **2022 / 05 / 18**  
### â±ï¸ kafka - spark ì»¤ë„¥í„° ì‚¬ìš© ì˜ˆì œ ë°ìŠ¤íŠ¸ (ì˜ˆì •)
- ì¹´í”„ì¹´ ì‹œì‘  
- ì¹´í”„ì¹´ ì‚¬ìš©í•´ì„œ ìŠ¤íŠ¸ë¦¬ë° ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ  
	âœ” ìŠ¤íŒŒí¬-ì¹´í”„ì¹´ ì»¤ë„¥í„° ì‚¬ìš©  
	âœ” ì¹´í”„ì¹´ë¡œ ë©”ì‹œì§€ ì „ì†¡  
	âœ” ì˜ˆì œ ì‹¤í–‰  
### â³ íŒŒì´í”„ë¼ì¸ ì‹œë‚˜ë¦¬ì˜¤ ìˆ˜ì • (ì§„í–‰ì¤‘)
### â–¶ ì˜¤í”ˆ apiì—ì„œ ë¡œê·¸ìŠ¤íƒœì‹œë¡œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²• ì°¾ê¸°(ì§„í–‰ì¤‘)  
- ë¬¸ì œ  
	âœ” ì˜ˆì œê°€ ì—†ì–´ì„œ config íŒŒì¼ ì‘ì„±ì´ ì–´ë µê³ , ì—ëŸ¬ ë°œìƒ  
### â³ ë¡œê·¸ìŠ¤íƒœì‹œ fitter ì½”ë“œ ì§œê¸° (ê¸°ìƒì •ë³´, ê°€ê²© ë° ë¶€ê°€ì •ë³´)  
- ê¸°ìƒì²­_ì§€ìƒ(ì¢…ê´€, ASOS) ì‹œê°„ìë£Œ ì¡°íšŒì„œë¹„ìŠ¤ config íŒŒì¼ ë‚´ìš©  
```
input {
  http_poller {
    urls => {
      url => "http://apis.data.go.kr/1360000/AsosHourlyInfoService/getWthrDataList?serviceKey=ì¸ì¦í‚¤&dataType=JSON\
      &numOfRows=50&pageNo=1&dataCd=ASOS&dateCd=HR&stnIds=108&endDt=20220517&endHh=20&startHh=13&startDt=20220517"}
    request_timeout => 60
    schedule => { cron => "0 0 0/1 * * *"} # 1ì‹œê°„ ë§ˆë‹¤ ì‹¤í–‰ 
    codec => json { charset => "UTF-8" }
  }
}


filter{
  split {
    field => "item"
  }
  
  mutate {
    remove_field => ["[item][m03Te]", "[item][m02Te]", [item][m01Te]", "[item][m005Te]", 
    "[item][dmstMtphNo]","[item][gndSttCd]","[item][vs]", "[item][lcsCh]","[item][clfmAbbrCd]"
    , "[item][dc10LmcsCa]" , "[item][dc10Tca]","[item][hr3Fhsc]", "[item][dsnw]"
    ,  "[item][icsr]", "[item][ss]", "[item][ssQcflag]", "[item][ps]", "[item][psQcflag]"
    , "[item][pageRow]", "[item][curPage]", "[item][trainCo]", "[item][ordkey]"
    , "[item][pa]", "[item][paQcflag]", "[item][td]", "[item][pv]"], "[item][hmQcflag]", "[item][wdQcflag]"]
    , "[item][wsQcflag]", "[item][rnQcflag]", "[item][taQcflag]", "[item][tsQcflg]"]
    #remove_field => ["errorMessage","@timestamp","@version"]
  }
}

output {
  elasticsearch {
    hosts => "ì—˜ë¼ìŠ¤í‹±ì„œì¹˜ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ” PCì˜ IP"
    index => "t_realtime_weather_list"
  }
}
```

- ë¬¸ì œ  
	âœ” runner - Logstash shut down. ìœ¼ë¡œ ìê¾¸ ë¡œê·¸ìŠ¤íƒœì‹œê°€ 5ì´ˆë§Œì— êº¼ì§  
	
### **2022 / 05 / 19**  
### â–¶ ìŠ¤íŒŒí¬ ìŠ¤íŠ¸ë¦¬ë° ì• í”Œë¦¬ì¼€ì´ì…˜ ì‘ì„± ì½”ë“œ ë¶„ì„ (ìì„¸í•˜ê²Œ í•œì¤„í•œì¤„)
### â–¶ ëª°ëë˜ ë©”ì„œë“œ ë° ìš©ì–´ ê³µë¶€  
- textFileStream ë©”ì„œë“œ  
	âœ”ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘í•œ ì‹œì  ì´í›„ì— í´ë”ë¡œ ë³µì‚¬ëœ íŒŒì¼ë§Œ ì²˜ë¦¬  
- splitAndSend.sh = ë¦¬ëˆ…ìŠ¤ ì…¸ìŠ¤í¬ë¦½íŠ¸  
	âœ” 50ë§Œ ê±´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì‹œìŠ¤í…œì— ìœ ì…í•˜ëŠ” ê²ƒì€ ë‹¤ì†Œ ë¹„í˜„ì‹¤ì .  
	âœ” ì…¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì´ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¥¼ ìƒì„±. HDFSë¡œ ë³µì‚¬í•˜ë ¤ë©´ ê¶Œí•œì´ ìˆì–´ì•¼ í•¨  
- DStream = ì´ì‚° ìŠ¤íŠ¸ë¦¼  
	âœ” ìŠ¤íŒŒí¬ ìŠ¤íŠ¸ë¦¬ë°ì˜ ê¸°ë³¸ ì¶”ìƒí™” ê°ì²´, ì…ë ¥ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ì¼ë ¨ì˜ RDD ì‹œí€€ìŠ¤   
- union ë©”ì„œë“œ  
	âœ” DStream ë³‘í•© ê°€ëŠ¥. ê·¼ë° ìš”ì†Œ íƒ€ì…ì´ ì„œë¡œ ë™ì¼í•´ì•¼ í•¨  
- shell ìŠ¤í¬ë¦½íŠ¸ = .sh íŒŒì¼  
	âœ” #!/bin/bashë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼  
	
### â–¶ ë¡œê·¸ ë°ì´í„° - ì¹´í”„ì¹´ - ìŠ¤íŒŒí¬ ìŠ¤íŠ¸ë¦¬ë° - ì¹´í”„ì¹´ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
1. shell ìŠ¤í¬ë¦½íŠ¸ë¡œ ë°ì´í„°ë¥¼ ì¹´í”„ì¹´ í† í”½ìœ¼ë¡œ ì „ì†¡  
2. ìŠ¤íŒŒí¬ ìŠ¤íŠ¸ë¦¬ë°ì€ í† í”½ì—ì„œ ë°ì´í„° ì½ê¸°  
3. ê° ì§€í‘œì˜ ê³„ì‚°ê²°ê³¼ëŠ” ë‹¤ì‹œ ë˜ ë‹¤ë¥¸ ì¹´í”„ì¹´ í† í”½ìœ¼ë¡œ ì „ì†¡  
4. ì¹´í”„ì¹´ëŠ” ì»¨ìŠˆë¨¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•´ì„œ ì§€í‘œ ê²°ê³¼ ìˆ˜ì‹  ë° ì¶œë ¥  

### **2022 / 05 / 20** 
### â³ ë¡œê·¸ ë°ì´í„° conf ì½”ë“œ ìˆ˜ì •  

### **2022 / 05 / 25** 
### â³ ì¹´í”„ì¹´ - ìŠ¤íŒŒí¬ (INFO shutting down (kafka.server.KafkaServer))  
- ë¬¸ì œ (kafka brokerê¸°ë™ì‹œ brokerIdê°€ ë‹¬ë¼ì„œ ê¸°ë™ì— ì‹¤íŒ¨)  
	âœ” config/server.properties íŒŒì¼ì— broker.idë¥¼ ìˆ˜ì • í›„ ì¬ì‹œì‘í–ˆë‹¤ê°€ ë‹¤ì‹œ broker.idë¥¼ ìˆ˜ì •  
	âœ” logsíŒŒì¼ ìœ„ì¹˜(ì˜ˆ, "/tmp/kafka-logs/meta.properties")ì— ìˆëŠ” meta.propertiesíŒŒì¼ì„ ì—´ì–´ì„œ broker.idë¥¼ ìˆ˜ì •í•˜ê³  ì €ì¥  
	
	``` 
	#A comma seperated list of directories under which to store log files
	log.dirs=/tmp/kafka-logs   
	
	
	 $ cat /tmp/kafka-logs/meta.properties
	#Tue May 24 17:53:38 KST 2022
	version=0
	broker.id=0   
	
	
	ì‹¤í–‰í•˜ê³  ë‹¤ì‹œ kafkaë¥¼ ì‹¤í–‰í•˜ë©´ ë” ì´ìƒ ìœ„ì˜ ì—ëŸ¬ëŠ” ë°œìƒí•˜ì§€ ì•ŠìŒ
	$ rm /tmp/kafka-logs/meta.properties 
	```  
- ìŠ¤íŒŒí¬ ì½”ë“œ ìˆ˜ì • (scala â†’ pyspark)  
	âŒ pyspark --master local[4] --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.1  
	âœ” pyspark --master local[4] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1  
	â˜…â˜…â˜…â˜… streamingì€ ì˜›ë‚ ë²„ì „ì—ì„œë§Œ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ì´ì œëŠ” sql ì‚¬ìš©í•  ê²ƒ â˜…â˜…â˜…â˜…   
	
```
pyspark --master local[4] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1

import os
os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1'

kafka_df = spark.readStream.   format("kafka").\
	option("subscribe", "test").\
	option("kafka.bootstrap.servers", "192.168.153.155:9092").\
	load()

query = kafka_df.writeStream.format("console").start()

kafka_df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")


# ì´ê²Œ ë¬¸ì œ
ds = kafka_df \
   .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
   .writeStream \
   .format("kafka") \
   .option("kafka.bootstrap.servers", "192.168.153.155:9092") \
   .option("topic", "test") \
   .start()

# .option("checkpointLocation", "/tmp") \ í•œì¤„ ì¶”ê°€. í•˜ì§€ë§Œ ì‹¤íŒ¨
ds = kafka_df \
   .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
   .writeStream \
   .format("kafka") \
   .option("kafka.bootstrap.servers", "192.168.153.155:9092") \
   .option("checkpointLocation", "/tmp") \
   .option("topic", "test") \
   .start()

# ì´ê²ƒë„ ì‹¤íŒ¨
ds = kafka_df \
   .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
   .writeStream \
   .format("console") \
   .start()

# ì´ê²ƒë„ ì‹¤íŒ¨
ds = kafka_df \
   .writeStream \
   .format("console") \
   .start()

# ì´ê²ƒë„ ì‹¤íŒ¨ì¸ë“¯??
ds = kafka_df \
   .writeStream \
   .format("console") \
   .option("checkpointLocation", "/tmp") \
   .start()

# ì´ê²ƒë„ ì‹¤íŒ¨
df.show()
df = spark.readStream.\
   format("kafka").\
   option("subscribe", "test").\
   option("kafka.bootstrap.servers", "192.168.153.155:9092").\
   option("startingOffsets", "earliest") \
   load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
df.printSchema()

# ì´ê²ƒë„ ì‹¤íŒ¨
df = spark.readStream.\
   format("kafka").\
   option("subscribe", "test").\
   option("kafka.bootstrap.servers", "192.168.153.155:9092").\
   option("startingOffsets", "earliest") .\
   load()
 df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
DataFrame[key: string, value: string]
df.printSchema()



# í¬ë§· ì½˜ì†”ë¡œ ë³€ê²½. ì´ê±° ë­”ê°€ ë°˜ì‘ìˆìŒ
query = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
     .writeStream \
     .format("console") \
     .start()

# ì±„í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì •. ì´ê²ƒë„ ì‹¤íŒ¨
query = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
     .writeStream \
     .format("console") \
     .option("checkpointLocation", "/tmp") \
     .start()

# ì´ê²Œ ì§„ì§œ??
query = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
     .writeStream \
     .format("console") \
     .option("checkpointLocation", "/tmp") \
     .start()
```
	
### **2022 / 05 / 26** 
### â³ ì¹´í”„ì¹´ - ìŠ¤íŒŒí¬
- ì¹´í”„ì¹´ì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì„œ ìŠ¤íŒŒí¬ì— ì¶œë ¥í•˜ëŠ”ê²ƒê¹Œì§€ ì™„ë£Œ
- ìŠ¤íŒŒí¬ ë‚´ì—ì„œ ê³„ì‚°í•œ ê²°ê³¼ í•˜ë‘¡(ë¡œì»¬)ì— ì €ì¥í•˜ê¸°
- ìŠ¤íŒŒí¬ 
	âœ” pyspark --master local[4]

```
>>> import os

ğŸ”” kafka-clients-3.1.0.jarì™€ commons-pool2-2.11.1.jarì™€ spark-token-provider-kafka-0-10_2.12-3.2.1.jar ë‹¤ìš´ë°›ê³  ì¶”ê°€
>>> os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,kafka-clients-3.1.0.jar,commons-pool2-2.11.1.jar,spark-token-provider-kafka-0-10_2.12-3.2.1.jar'
>>> 
>>> 
>>> df = spark.readStream.\
...   format("kafka").\
...   option("subscribe", "test").\
...   option("kafka.bootstrap.servers", "192.168.153.155:9092").\
...   option("startingOffsets", "earliest") .\
...   load()

>>> df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
DataFrame[key: string, value: string]
>>> df.printSchema()
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

>>> 
>>> query = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
...     .writeStream \
...     .format("console") \
...     .option("checkpointLocation", "/tmp") \
...     .start()
22/05/26 10:28:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
>>> 
>>> 
>>> type(query)
<class 'pyspark.sql.streaming.StreamingQuery'>
>>> df.show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/spark/python/pyspark/sql/dataframe.py", line 494, in show
    print(self._jdf.showString(n, 20, vertical))
  File "/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: Queries with streaming sources must be executed with writeStream.start();
kafka
>>> 
>>> 
>>> df.show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/spark/python/pyspark/sql/dataframe.py", line 494, in show
    print(self._jdf.showString(n, 20, vertical))
  File "/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: Queries with streaming sources must be executed with writeStream.start();
kafka
>>> df.select(df.value)
DataFrame[value: binary]
>>> df.select(df.topic)
DataFrame[topic: string]
>>> query.awaitTermination()
-------------------------------------------                                     
Batch: 0
-------------------------------------------
+---+-----+
|key|value|
+---+-----+
+---+-----+

-------------------------------------------                                     
Batch: 1
-------------------------------------------
+----+-----+
| key|value|
+----+-----+
|null|     |
|null|     |
|null|     |
+----+-----+

-------------------------------------------                                     
Batch: 2
-------------------------------------------
+----+-----+
| key|value|
+----+-----+
|null| test|
|null| qwer|
+----+-----+


```

### **2022 / 05 / 31** 
### â³ ì¹´í”„ì¹´ - ìŠ¤íŒŒí¬ ë¬¸ì œìƒí™©
1. ì¹´í”„ì¹´ ì„œë²„ ì‹¤í–‰ ì•ˆë¨ (ê³„ì† ì…§ë‹¤ìš´ë˜ê³ , ipê°€ ì‚¬ë¼ì§€ëŠ”ë° ì´ìœ  ëª¨ë¥´ê² ìŒ)
2. 6ì›” 1ì¼ í…ŒìŠ¤íŠ¸ ê²¸ ì‹¤í–‰í•  ë•Œ ì–´ë–¤ ì»´ìœ¼ë¡œ ì›ê²©ì ‘ì†? ë¡œê·¸ ë°ì´í„°ê°€ ì¹´í”„ì¹´ë¡œ ë“¤ì–´ì™€ë„ í˜„ì¬ ì—°ê²°ëœ ì»´í“¨í„°ëŠ” ì‹¤í–‰ ë¶ˆê°€
3. ìŠ¤íŒŒí¬ì—ì„œ ì¹´í”„ì¹´ í˜¸ì¶œí•˜ëŠ” ì½”ë“œë‘ ë°ì´í„° ì—°ì‚°í•˜ëŠ” ì½”ë“œë¥¼ í•˜ë‚˜ë¡œ í•©ì³ë´¤ëŠ”ë° 1ë²ˆ ë•Œë¬¸ì— í…ŒìŠ¤íŠ¸ ëª» í•´ë´„

### **2022 / 06 / 03** 
### â³ ì¹´í”„ì¹´ì—ì„œ ë°›ì€ ë°ì´í„° ìŠ¤íŒŒí¬ì—ì„œ ì‹¤í–‰í•  ë•Œ ë¬¸ì œì‚¬í•­
- df.show í•˜ê±°ë‚˜ .saveë¡œ ì‹¤í–‰í•˜ë©´ ì—„ì²­ ê¸´ ì—ëŸ¬ ë©”ì„¸ì§€ ì¶œë ¥ â†’ ê¼¼ê¼¼íˆ ì‚´í´ë´ì„œ java.net.UnknownHostException ì—ëŸ¬ í™•ì¸í•˜ê³  í•´ê²° 
   
![image](https://user-images.githubusercontent.com/74638968/171814004-22a433e9-cedd-4ced-b640-6d69311e9453.png) 

	âœ” vi /etc/hosts ì— ì¹´í”„ì¹´ ë¸Œë¡œì»¤ ipë‘ ì¹´í”„ì¹´ hostname ì¶”ê°€  
	```
	âœ” 192.168.182.19 kafka-646c48c49b-69tvs  
	âŒ 127.0.0.1 kafka-646c48c49b-69tvs  
	âŒ 127.0.0.1 hadoop-spark-zeppelin-665894bc4d-4hdxq  
	```

### â³ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¹´í”„ì¹´ í† í”½ì—ì„œ ë°ì´í„° ë°›ì•„ì„œ ìŠ¤íŒŒí¬ì— ì¶œë ¥
```
spark-shell --master local[4] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,org.apache.kafka:kafka_2.13:3.1.0 --jars kafka-clients-3.1.0.jar,commons-pool2-2.11.1.jar,spark-token-provider-kafka-0-10_2.12-3.2.1.jar

import org.apache.spark._
import org.apache.spark.sql.Column
import org.apache.spark.streaming._

val df = spark.read.format("kafka").
    option("subscribe", "logdata_test2").
    option("kafka.bootstrap.servers", "192.168.182.19:9092").
    load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")

val query = df.selectExpr("CAST(value AS STRING)").
    write.
    format("console").
    option("checkpointLocation", "/tmp/checkpoint").
    save()
```
![image](https://user-images.githubusercontent.com/74638968/171819029-42e03846-f063-48fd-be9c-12e78cd26644.png)
![df show](https://user-images.githubusercontent.com/74638968/171994289-231adc3f-d39e-4cdf-95c8-6475970a8195.PNG)
------------------------------------------------------------------------------------------------------------------

### **2022 / 06 / 04** 
### â³ ì¹´í”„ì¹´ì—ì„œ ë°›ì€ ë°ì´í„° ìŠ¤íŒŒí¬ì—ì„œ ê³„ì‚°í•  ë•Œ ë¬¸ì œì   

âœ” ì¹´í”„ì¹´ í† í”½ ë°ì´í„°(json íŒŒì¼)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ  (split is not a member of org.apache.spark.sql.Row) ë°ì´í„° ì •ì œ  
```
âŒ val s = line.value.split(",")  

import java.text.SimpleDateFormat 
val orders = df.select($"value").flatMap(line => {
    val dateFormat = new SimpleDateFormat("hh:mm:ss")
    val s = line.value.split(",")
    try {
      List(Order(new Timestamp(dateFormat.parse(s(0)).getTime()),
    s(1).toLong, s(2).toLong, s(3), s(4).toInt))
    }  
    catch {
      case e : Throwable => println("Wrong line format ("+e+"): "+line)
      List()
    }
})
```

### **2022 / 06 / 05** 
### â³ ìŠ¤íŒŒí¬ì—ì„œ binary í˜•íƒœì˜ ë°ì´í„° ì½ê³  ë³€í™˜ 
![image](https://user-images.githubusercontent.com/74638968/172055027-6f47d530-e2c0-4b08-a05b-6c38d96fbea8.png)

```
val df = spark.read.format("kafka").
    option("subscribe", "logdata_test2").
    option("kafka.bootstrap.servers", "192.168.182.19:9092").
    load()

### ì—¬ê¸°ì„œ "value" ì€ íŒŒì‹±ëœ ê°’ì´ ë“¤ì–´ê°ˆ ìƒˆ column ì˜ ì´ë¦„.
### df("value") ëŠ” binary ê°’ì´ ë“¤ì–´ìˆëŠ” dataframeì˜ column ì´ë¦„ (df ê°ì²´ì˜ "value" í•„ë“œ)
### ê²°ê³¼ì ìœ¼ë¡œ ê¸°ì¡´ì˜ value ê°’ì„ ìƒˆë¡œìš´ value ê°€ ë®ì–´ì“°ëŠ” í˜•íƒœ

val toStr = udf((payload: Array[Byte]) => new String(payload))
val parsing = df.withColumn("value", toStr(df("value")))
```

![image](https://user-images.githubusercontent.com/74638968/172054978-7ee03b20-5320-4416-b11b-777b72a19720.png)

### **2022 / 06 / 07** 
## â–¶ ìŠ¤íŒŒí¬ ìµœì¢… ì½”ë“œ  

### âœ” ìŠ¤íŒŒí¬ shell ì‹¤í–‰  
	spark-shell --master local[4] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,org.apache.kafka:kafka_2.13:3.1.0 --jars kafka-clients-3.1.0.jar,commons-pool2-2.11.1.jar,spark-token-provider-kafka-0-10_2.12-3.2.1.jar  
![image](https://user-images.githubusercontent.com/74638968/172333109-f0168567-15b6-4cb0-b1fa-ddf10153df73.png)  
	
### âœ” 0. import ëª¨ìŒ
```
import org.apache.spark._
import org.apache.spark.sql.Row
import org.apache.spark.sql.Column
import org.apache.spark.streaming._
import spark.implicits._
import org.apache.spark.sql.functions.get_json_object
import org.apache.spark.sql.types._
```
![image](https://user-images.githubusercontent.com/74638968/172333210-3a5d17ba-0e85-4083-8da0-26bffe32a8a6.png)  

### âœ” 1. ì¹´í”„ì¹´ í† í”½ì— ì €ì¥ëœ ë¡œê·¸ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
```
val df = spark.read.format("kafka").
    option("subscribe", "final_logdata").
    option("kafka.bootstrap.servers", "192.168.182.19:9092").
    load()
df.show(false)
```
![image](https://user-images.githubusercontent.com/74638968/172333440-e474fac8-b17a-4e2b-8f8a-2e0a69fd1a61.png)  
![image](https://user-images.githubusercontent.com/74638968/172333593-51eca289-f404-4400-8973-a857f4ed877d.png) 

### âœ” 2. binary í˜•íƒœ(ìš°ë¦¬ê°€ ë³¼ ìˆ˜ ì—†ìŒ)ì˜ ë°ì´í„°ë¥¼ stringìœ¼ë¡œ ë°”ê¾¸ê³ , í•„ìš”ì—†ëŠ” ì¹¼ëŸ¼ ì‚­ì œí•˜ê¸°(key,topic, partition, timestampType, offset, timestamp)
```
val toStr = udf((payload: Array[Byte]) => new String(payload))
val parsing = df.withColumn("value", toStr(df("value")))
val df2 = parsing.drop("key", "topic", "partition", "timestampType", "offset", "timestamp")
df2.printSchema
df2.show(false)
```
![image](https://user-images.githubusercontent.com/74638968/172334323-9fe1cd9d-0ab8-4ec4-b9ce-71f4cabe86a8.png)  
![image](https://user-images.githubusercontent.com/74638968/172334947-0c25eb97-cd11-4863-8bbc-d28fb96f775f.png)  

### âœ” 3. êµ¬ì¡°ì²´ ìƒì„±
``` 
### Spark Convert JSON Column to struct Column
### from_json(Column jsonStringcolumn, StructType schema)ë¥¼ ì´ìš©í•˜ì—¬ spark dataframe ì—´ì˜ json ë¬¸ìì—´ì„ êµ¬ì¡°ì²´ ìœ í˜•ìœ¼ë¡œ ë³€í™˜
// ê°€ì¥ ë¨¼ì € json ë¬¸ìì—´ì— ëŒ€í•œ structType ìƒì„±
import org.apache.spark.sql.types.{StringType, StructType, TimestampType, LongType, IntegerType}

// val schema = new StructType().add("time", TimestampType, true).add("order_num", LongType, true).add("user_num", LongType, true).add("item", StringType, true).add("item_ea", IntegerType, true)

// ìœ„ì˜ ì½”ë“œì²˜ëŸ¼ ê° ì¹¼ëŸ¼ë³„ë¡œ ë°ì´í„° íƒ€ì…ì„ ì§€ì •í•˜ë ¤ê³  í–ˆìœ¼ë‚˜ ê·¸ë ‡ê²Œ í•˜ë‹ˆê¹Œ nullê°’ì´ ë˜ì–´ë²„ë¦¼
// ê·¸ë˜ì„œ ì „ì²´ stringTypeìœ¼ë¡œ ì§€ì •
val schema = new StructType().add("time", StringType, true).add("order_num", StringType, true).add("user_num", StringType, true).add("item", StringType, true).add("item_ea", StringType, true)

// from_jsonì—ì„œ ìœ„ì˜ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
val df3 = df2.withColumn("value",from_json(col("value"),schema))
df3.printSchema()
df3.show(false)
```
![image](https://user-images.githubusercontent.com/74638968/172335264-2a022248-6e8f-45de-adde-7cb06df9f013.png)  
![image](https://user-images.githubusercontent.com/74638968/172335342-50749c64-099d-4f19-bf77-a7d17e6d75fa.png)  


### âœ” 4. spark json ì—´ì„ ì—¬ëŸ¬ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜ 
```
// value êµ¬ì¡°ì²´ë¥¼ ê°œë³„ ì—´ë¡œ ë³€í™˜
val df4=df3.select(col("value.*"))
df4.printSchema()
df4.show(false)
```
![image](https://user-images.githubusercontent.com/74638968/172335541-08996c85-d0a0-4d98-a0ab-2eaf5d0d2b26.png)  
![image](https://user-images.githubusercontent.com/74638968/172335652-fe39496c-d051-4ef7-8788-38f5f9ec5365.png)  

### âœ” 5. í’ˆëª©ë³„ë¡œ êµ¬ë§¤ ìˆ˜ëŸ‰ì— ë§ëŠ” ì •ë ¬ (spark sql window function)
```
// spark window ranking functions
// row_number() window function is used to give the sequential row number starting from 1 to the result of each window partition.
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

// item ë³„ë¡œ ìˆ˜ëŸ‰ ê°¯ìˆ˜ ì •ë ¬í•˜ê¸°
val windowitem  = Window.partitionBy("item").orderBy("item_ea")
val df5 = df4.withColumn("row_number",row_number.over(windowitem))
df5.show

// item ë³„ë¡œ ì¶œë ¥ (í’ˆëª©ì€ í•˜ë‚˜ì”©ë§Œ ì¶œë ¥)
val windowitemAgg  = Window.partitionBy("item")

val df6 = df5.withColumn("row",row_number.over(windowitem)).withColumn("sum", sum(col("item_ea")).over(windowitemAgg)).withColumn("min", min(col("item_ea")).over(windowitemAgg)).withColumn("max", max(col("item_ea")).over(windowitemAgg)).where(col("row")===1).select("item","sum","min","max").orderBy(desc("sum"))
df6.show
```
![image](https://user-images.githubusercontent.com/74638968/172335842-0e9d98c9-b467-4eb5-be27-7a88e0e38df9.png)  
![image](https://user-images.githubusercontent.com/74638968/172335989-afc3aa2c-91a0-429b-bf9e-393aa48601d0.png)  
![image](https://user-images.githubusercontent.com/74638968/172336130-6c8b29f0-ada2-4580-b6d2-b71716ebc40f.png)  
